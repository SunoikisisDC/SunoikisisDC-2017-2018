{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech tagging and lemmatisation with üêç\n",
    "\n",
    "See information on the [Sunoikisis Wiki](https://github.com/SunoikisisDC/SunoikisisDC-2017-2018/wiki/Python-2:-Part-of-Speech-tagging-and-lemmatisation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.83'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cltk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "grk_corpus_importer = CorpusImporter('greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greek_software_tlgu',\n",
       " 'greek_text_perseus',\n",
       " 'phi7',\n",
       " 'tlg',\n",
       " 'greek_proper_names_cltk',\n",
       " 'greek_models_cltk',\n",
       " 'greek_treebank_perseus',\n",
       " 'greek_lexica_perseus',\n",
       " 'greek_training_set_sentence_cltk',\n",
       " 'greek_word2vec_cltk',\n",
       " 'greek_text_lacus_curtius',\n",
       " 'greek_text_first1kgreek']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grk_corpus_importer.list_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 100% 143.79 MiB | 4.02 MiB/s s \r"
     ]
    }
   ],
   "source": [
    "grk_corpus_importer.import_corpus('greek_text_perseus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `cltk.corpus.latin.latinlibrary` is a shortcut for several things, and there is nothing comparable (yet) for Greek (see [source code](https://github.com/cltk/cltk/blob/master/cltk/corpus/latin/__init__.py))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_corpus_importer = CorpusImporter('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 100% 35.50 MiB | 5.91 MiB/s \r"
     ]
    }
   ],
   "source": [
    "la_corpus_importer.import_corpus('latin_text_latin_library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.latin import latinlibrary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB**: Disclaimer about what the library does behind the scenes when one imports the submodule `latinlibrary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "amicitia_words = latinlibrary.words('cicero/amic.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11618"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amicitia_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get `n` number of tokens from this text by using the *slice notation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cicero',\n",
       " ':',\n",
       " 'de',\n",
       " 'Amicitia',\n",
       " 'M.',\n",
       " 'TVLLI',\n",
       " 'CICERONIS',\n",
       " 'LAELIVS',\n",
       " 'DE',\n",
       " 'AMICITIA']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first ten tokens\n",
    "amicitia_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or the last token\n",
    "amicitia_words[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also count occurrences by using the `count()` method and passing as parameter the token we want to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amicitia_words.count('et')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amicitia_words.count('amicitia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look to the `type` of the variable `amicitia_words` where we loaded the content of Cicero's *De Amicitia*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.util.StreamBackedCorpusView"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(amicitia_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on StreamBackedCorpusView in module nltk.corpus.reader.util object:\n",
      "\n",
      "class StreamBackedCorpusView(nltk.collections.AbstractLazySequence)\n",
      " |  A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      " |  it can be accessed by index, iterated over, etc.  However, the\n",
      " |  tokens are only constructed as-needed -- the entire corpus is\n",
      " |  never stored in memory at once.\n",
      " |  \n",
      " |  The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      " |  a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      " |  and a block reader.  A \"block reader\" is a function that reads\n",
      " |  zero or more tokens from a stream, and returns them as a list.  A\n",
      " |  very simple example of a block reader is:\n",
      " |  \n",
      " |      >>> def simple_block_reader(stream):\n",
      " |      ...     return stream.readline().split()\n",
      " |  \n",
      " |  This simple block reader reads a single line at a time, and\n",
      " |  returns a single token (consisting of a string) for each\n",
      " |  whitespace-separated substring on the line.\n",
      " |  \n",
      " |  When deciding how to define the block reader for a given\n",
      " |  corpus, careful consideration should be given to the size of\n",
      " |  blocks handled by the block reader.  Smaller block sizes will\n",
      " |  increase the memory requirements of the corpus view's internal\n",
      " |  data structures (by 2 integers per block).  On the other hand,\n",
      " |  larger block sizes may decrease performance for random access to\n",
      " |  the corpus.  (But note that larger block sizes will *not*\n",
      " |  decrease performance for iteration.)\n",
      " |  \n",
      " |  Internally, ``CorpusView`` maintains a partial mapping from token\n",
      " |  index to file position, with one entry per block.  When a token\n",
      " |  with a given index *i* is requested, the ``CorpusView`` constructs\n",
      " |  it as follows:\n",
      " |  \n",
      " |    1. First, it searches the toknum/filepos mapping for the token\n",
      " |       index closest to (but less than or equal to) *i*.\n",
      " |  \n",
      " |    2. Then, starting at the file position corresponding to that\n",
      " |       index, it reads one block at a time using the block reader\n",
      " |       until it reaches the requested token.\n",
      " |  \n",
      " |  The toknum/filepos mapping is created lazily: it is initially\n",
      " |  empty, but every time a new block is read, the block's\n",
      " |  initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      " |  map has one entry per block.)\n",
      " |  \n",
      " |  In order to increase efficiency for random access patterns that\n",
      " |  have high degrees of locality, the corpus view may cache one or\n",
      " |  more blocks.\n",
      " |  \n",
      " |  :note: Each ``CorpusView`` object internally maintains an open file\n",
      " |      object for its underlying corpus file.  This file should be\n",
      " |      automatically closed when the ``CorpusView`` is garbage collected,\n",
      " |      but if you wish to close it manually, use the ``close()``\n",
      " |      method.  If you access a ``CorpusView``'s items after it has been\n",
      " |      closed, the file object will be automatically re-opened.\n",
      " |  \n",
      " |  :warning: If the contents of the file are modified during the\n",
      " |      lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      " |      is undefined.\n",
      " |  \n",
      " |  :warning: If a unicode encoding is specified when constructing a\n",
      " |      ``CorpusView``, then the block reader may only call\n",
      " |      ``stream.seek()`` with offsets that have been returned by\n",
      " |      ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      " |      relative offsets, or with offsets based on string lengths, may\n",
      " |      lead to incorrect behavior.\n",
      " |  \n",
      " |  :ivar _block_reader: The function used to read\n",
      " |      a single block from the underlying file stream.\n",
      " |  :ivar _toknum: A list containing the token index of each block\n",
      " |      that has been processed.  In particular, ``_toknum[i]`` is the\n",
      " |      token index of the first token in block ``i``.  Together\n",
      " |      with ``_filepos``, this forms a partial mapping between token\n",
      " |      indices and file positions.\n",
      " |  :ivar _filepos: A list containing the file position of each block\n",
      " |      that has been processed.  In particular, ``_toknum[i]`` is the\n",
      " |      file position of the first character in block ``i``.  Together\n",
      " |      with ``_toknum``, this forms a partial mapping between token\n",
      " |      indices and file positions.\n",
      " |  :ivar _stream: The stream used to access the underlying corpus file.\n",
      " |  :ivar _len: The total number of tokens in the corpus, if known;\n",
      " |      or None, if the number of tokens is not yet known.\n",
      " |  :ivar _eofpos: The character position of the last character in the\n",
      " |      file.  This is calculated when the corpus view is initialized,\n",
      " |      and is used to decide when the end of file has been reached.\n",
      " |  :ivar _cache: A cache of the most recently read block.  It\n",
      " |     is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      " |     start_toknum is the token index of the first token in the block;\n",
      " |     end_toknum is the token index of the first token not in the\n",
      " |     block; and tokens is a list of the tokens in the block.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StreamBackedCorpusView\n",
      " |      nltk.collections.AbstractLazySequence\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Return a list concatenating self with other.\n",
      " |  \n",
      " |  __getitem__(self, i)\n",
      " |      Return the *i* th token in the corpus file underlying this\n",
      " |      corpus view.  Negative indices and spans are both supported.\n",
      " |  \n",
      " |  __init__(self, fileid, block_reader=None, startpos=0, encoding='utf8')\n",
      " |      Create a new corpus view, based on the file ``fileid``, and\n",
      " |      read with ``block_reader``.  See the class documentation\n",
      " |      for more information.\n",
      " |      \n",
      " |      :param fileid: The path to the file that is read by this\n",
      " |          corpus view.  ``fileid`` can either be a string or a\n",
      " |          ``PathPointer``.\n",
      " |      \n",
      " |      :param startpos: The file position at which the view will\n",
      " |          start reading.  This can be used to skip over preface\n",
      " |          sections.\n",
      " |      \n",
      " |      :param encoding: The unicode encoding that should be used to\n",
      " |          read the file's contents.  If no encoding is specified,\n",
      " |          then the file's contents will be read as a non-unicode\n",
      " |          string (i.e., a str).\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of tokens in the corpus file underlying this\n",
      " |      corpus view.\n",
      " |  \n",
      " |  __mul__(self, count)\n",
      " |      Return a list concatenating self with itself ``count`` times.\n",
      " |  \n",
      " |  __radd__(self, other)\n",
      " |      Return a list concatenating other with self.\n",
      " |  \n",
      " |  __rmul__(self, count)\n",
      " |      Return a list concatenating self with itself ``count`` times.\n",
      " |  \n",
      " |  close(self)\n",
      " |      Close the file stream associated with this corpus view.  This\n",
      " |      can be useful if you are worried about running out of file\n",
      " |      handles (although the stream should automatically be closed\n",
      " |      upon garbage collection of the corpus view).  If the corpus\n",
      " |      view is accessed after it is closed, it will be automatically\n",
      " |      re-opened.\n",
      " |  \n",
      " |  iterate_from(self, start_tok)\n",
      " |      Return an iterator that generates the tokens in the corpus\n",
      " |      file underlying this corpus view, starting at the token number\n",
      " |      ``start``.  If ``start>=len(self)``, then this iterator will\n",
      " |      generate no tokens.\n",
      " |  \n",
      " |  read_block(self, stream)\n",
      " |      Read a block from the input stream.\n",
      " |      \n",
      " |      :return: a block of tokens from the input stream\n",
      " |      :rtype: list(any)\n",
      " |      :param stream: an input stream\n",
      " |      :type stream: stream\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  fileid\n",
      " |      The fileid of the file that is accessed by this view.\n",
      " |      \n",
      " |      :type: str or PathPointer\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.collections.AbstractLazySequence:\n",
      " |  \n",
      " |  __contains__(self, value)\n",
      " |      Return true if this list contains ``value``.\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, other, NotImplemented=NotImplemented)\n",
      " |      Return a >= b.  Computed by @total_ordering from (not a < b).\n",
      " |  \n",
      " |  __gt__(self, other, NotImplemented=NotImplemented)\n",
      " |      Return a > b.  Computed by @total_ordering from (not a < b) and (a != b).\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      :raise ValueError: Corpus view objects are unhashable.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return an iterator that generates the tokens in the corpus\n",
      " |      file underlying this corpus view.\n",
      " |  \n",
      " |  __le__(self, other, NotImplemented=NotImplemented)\n",
      " |      Return a <= b.  Computed by @total_ordering from (a < b) or (a == b).\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return a string representation for this corpus view that is\n",
      " |      similar to a list's representation; but if it would be more\n",
      " |      than 60 characters long, it is truncated.\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  count(self, value)\n",
      " |      Return the number of times this list contains ``value``.\n",
      " |  \n",
      " |  index(self, value, start=None, stop=None)\n",
      " |      Return the index of the first occurrence of ``value`` in this\n",
      " |      list that is greater than or equal to ``start`` and less than\n",
      " |      ``stop``.  Negative start and stop values are treated like negative\n",
      " |      slice bounds -- i.e., they count from the end of the list.\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |      Return a string representation for this corpus view that is\n",
      " |      similar to a list's representation; but if it would be more\n",
      " |      than 60 characters long, it is truncated.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.collections.AbstractLazySequence:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(amicitia_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latin\n",
    "\n",
    "#### CLTK taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "CLTK linguistics models not available for unigram.pickle.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-002e391a20e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPOSTag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOSTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_ngram_123_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Gallia est omnis divisa in partes tres'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/cltk/tag/pos.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;34m\"\"\"Setup variables.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_taggers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_language_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_language_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=no-self-use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/cltk/tag/pos.py\u001b[0m in \u001b[0;36m_setup_language_variables\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtagger_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagger_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0;34m'CLTK linguistics models not available for {0}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagger_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mtagger_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtagger_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtagger_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: CLTK linguistics models not available for unigram.pickle."
     ]
    }
   ],
   "source": [
    "from cltk.tag.pos import POSTag\n",
    "tagger = POSTag('latin')\n",
    "tagger.tag_ngram_123_backoff('Gallia est omnis divisa in partes tres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latin_text_perseus',\n",
       " 'latin_treebank_perseus',\n",
       " 'latin_text_latin_library',\n",
       " 'phi5',\n",
       " 'phi7',\n",
       " 'latin_proper_names_cltk',\n",
       " 'latin_models_cltk',\n",
       " 'latin_pos_lemmata_cltk',\n",
       " 'latin_treebank_index_thomisticus',\n",
       " 'latin_lexica_perseus',\n",
       " 'latin_training_set_sentence_cltk',\n",
       " 'latin_word2vec_cltk',\n",
       " 'latin_text_antique_digiliblt',\n",
       " 'latin_text_corpus_grammaticorum_latinorum',\n",
       " 'latin_text_poeti_ditalia']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_corpus_importer.list_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_corpus_importer.import_corpus('latin_models_cltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = POSTag('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cltk.tag.pos.POSTag at 0x113476d68>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('91', 'Unk'), ('91', None), ('91', 'A-P---FN-')),\n",
       " (('92', 'Unk'), ('92', None), ('92', 'N-P---FN-')),\n",
       " (('93', 'Unk'), ('93', None), ('93', 'A-P---FN-')),\n",
       " (('94', 'Unk'), ('94', None), ('94', 'N-P---FN-')),\n",
       " (('95', 'Unk'), ('95', None), ('95', 'A-P---FN-')),\n",
       " (('96', 'Unk'), ('96', None), ('96', 'N-P---FN-')),\n",
       " (('97', 'Unk'), ('97', None), ('97', 'A-P---FN-')),\n",
       " (('98', 'Unk'), ('98', None), ('98', 'N-P---FN-')),\n",
       " (('99', 'Unk'), ('99', None), ('99', 'A-P---FN-')),\n",
       " (('100', 'Unk'), ('100', None), ('100', 'N-P---FN-')),\n",
       " (('101', 'Unk'), ('101', None), ('101', 'A-P---FN-')),\n",
       " (('102', 'Unk'), ('102', None), ('102', 'N-P---FN-')),\n",
       " (('103', 'Unk'), ('103', None), ('103', 'A-P---FN-')),\n",
       " (('104', 'Unk'), ('104', None), ('104', 'N-P---FN-')),\n",
       " (('[', 'U--------'), ('[', 'U--------'), ('[', 'U--------')),\n",
       " (('1', 'Unk'), ('1', None), ('1', 'N-S---MV-')),\n",
       " ((']', 'U--------'), (']', 'U--------'), (']', 'U--------')),\n",
       " (('Q', 'Unk'), ('Q', None), ('Q', 'N-S---MV-')),\n",
       " (('.', 'U--------'), ('.', 'U--------'), ('.', 'U--------')),\n",
       " (('Mucius', 'Unk'), ('Mucius', None), ('Mucius', 'D--------')),\n",
       " (('augur', 'Unk'), ('augur', None), ('augur', 'D--------')),\n",
       " (('multa', 'A-P---NA-'), ('multa', 'A-P---NA-'), ('multa', 'A-P---NA-')),\n",
       " (('narrare', 'V--PNA---'),\n",
       "  ('narrare', 'V--PNA---'),\n",
       "  ('narrare', 'V--PNA---')),\n",
       " (('de', 'R--------'), ('de', 'R--------'), ('de', 'R--------')),\n",
       " (('C', 'Unk'), ('C', None), ('C', '---------')),\n",
       " (('.', 'U--------'), ('.', 'U--------'), ('.', 'U--------')),\n",
       " (('Laelio', 'Unk'), ('Laelio', None), ('Laelio', 'A-S---NB-')),\n",
       " (('socero', 'Unk'), ('socero', None), ('socero', 'N-S---NB-')),\n",
       " (('suo', 'A-S---NB-'), ('suo', 'A-S---NB-'), ('suo', 'A-S---NB-')),\n",
       " (('memoriter', 'Unk'), ('memoriter', None), ('memoriter', 'D--------')),\n",
       " (('et', 'C--------'), ('et', 'C--------'), ('et', 'C--------')),\n",
       " (('iucunde', 'Unk'), ('iucunde', None), ('iucunde', 'D--------')),\n",
       " (('solebat', 'V3SIIA---'),\n",
       "  ('solebat', 'V3SIIA---'),\n",
       "  ('solebat', 'V3SIIA---')),\n",
       " (('nec', 'C--------'), ('nec', 'C--------'), ('nec', 'C--------')),\n",
       " (('dubitare', 'Unk'), ('dubitare', None), ('dubitare', 'V--PNA---')),\n",
       " (('illum', 'P-S---MA-'), ('illum', 'P-S---MA-'), ('illum', 'P-S---MA-')),\n",
       " (('in', 'R--------'), ('in', 'R--------'), ('in', 'R--------')),\n",
       " (('omni', 'A-S---MB-'), ('omni', 'A-S---FB-'), ('omni', 'A-S---FB-')),\n",
       " (('sermone', 'N-S---MB-'),\n",
       "  ('sermone', 'N-S---MB-'),\n",
       "  ('sermone', 'N-S---FB-')),\n",
       " (('appellare', 'V--PNA---'),\n",
       "  ('appellare', 'V--PNA---'),\n",
       "  ('appellare', 'V--PNA---')),\n",
       " (('sapientem', 'Unk'), ('sapientem', None), ('sapientem', 'N-S---FA-')),\n",
       " ((';', 'Unk'), (';', None), (';', 'U--------')),\n",
       " (('ego', 'P-S---MN-'), ('ego', 'P-S---MN-'), ('ego', 'P-S---MN-')),\n",
       " (('autem', 'C--------'), ('autem', 'C--------'), ('autem', 'C--------')),\n",
       " (('a', 'R--------'), ('a', 'R--------'), ('a', 'R--------')),\n",
       " (('patre', 'N-S---MB-'), ('patre', 'N-S---MB-'), ('patre', 'N-S---MB-')),\n",
       " (('ita', 'D--------'), ('ita', 'D--------'), ('ita', 'D--------')),\n",
       " (('eram', 'V1SIIA---'), ('eram', 'V1SIIA---'), ('eram', 'N-S---FA-')),\n",
       " (('deductus', 'T-SRPPMN-'),\n",
       "  ('deductus', 'T-SRPPMN-'),\n",
       "  ('deductus', 'T-SRPPMN-')),\n",
       " (('ad', 'R--------'), ('ad', 'R--------'), ('ad', 'R--------')),\n",
       " (('Scaevolam', 'Unk'), ('Scaevolam', None), ('Scaevolam', 'N-S---FA-')),\n",
       " (('sumpta', 'Unk'), ('sumpta', None), ('sumpta', 'T-SRPPFN-'))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(\n",
    "    tagger.tag_tnt(\" \".join([str(w) for w in amicitia_words[100:150]])),\n",
    "    tagger.tag_ngram_123_backoff(\" \".join([str(w) for w in amicitia_words[100:150]])),\n",
    "    tagger.tag_crf(\" \".join([str(w) for w in amicitia_words[100:150]]))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TreeTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treetagger import TreeTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TREETAGGER_HOME\"] = \"/Users/rromanello/tree-tagger/cmd/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TreeTagger(language=\"latin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['91', 'ADJ:NUM', '@card@'],\n",
       " ['92', 'ADJ:NUM', '@card@'],\n",
       " ['93', 'ADJ:NUM', '@card@'],\n",
       " ['94', 'ADJ:NUM', '@card@'],\n",
       " ['95', 'ADJ:NUM', '@card@'],\n",
       " ['96', 'ADJ:NUM', '@card@'],\n",
       " ['97', 'ADJ:NUM', '@card@'],\n",
       " ['98', 'ADJ:NUM', '@card@'],\n",
       " ['99', 'ADJ:NUM', '@card@'],\n",
       " ['100', 'ADJ:NUM', '@card@'],\n",
       " ['101', 'ADJ:NUM', '@card@'],\n",
       " ['102', 'ADJ:NUM', '@card@'],\n",
       " ['103', 'ADJ:NUM', '@card@'],\n",
       " ['104', 'ADJ:NUM', '@card@'],\n",
       " ['[', 'PUN', '['],\n",
       " ['1', 'ADJ:NUM', '@card@'],\n",
       " [']', 'PUN', ']'],\n",
       " ['Q.', 'ABBR', 'Q.'],\n",
       " ['Mucius', 'ADJ', '<unknown>'],\n",
       " ['augur', 'N:nom', 'augur'],\n",
       " ['multa', 'ADJ', 'multus'],\n",
       " ['narrare', 'V:INF', 'narro'],\n",
       " ['de', 'PREP', 'de'],\n",
       " ['C.', 'ABBR', 'C.'],\n",
       " ['Laelio', 'N:abl', '<unknown>'],\n",
       " ['socero', 'N:abl', 'socer'],\n",
       " ['suo', 'POSS', 'suus'],\n",
       " ['memoriter', 'ADV', 'memoriter'],\n",
       " ['et', 'CC', 'et'],\n",
       " ['iucunde', 'ADJ', '<unknown>'],\n",
       " ['solebat', 'V:IND', 'soleo'],\n",
       " ['nec', 'CC', 'nec'],\n",
       " ['dubitare', 'V:INF', 'dubito'],\n",
       " ['illum', 'DIMOS', 'ille'],\n",
       " ['in', 'PREP', 'in'],\n",
       " ['omni', 'PRON', 'omnis'],\n",
       " ['sermone', 'N:abl', 'sermo'],\n",
       " ['appellare', 'V:INF', 'appello'],\n",
       " ['sapientem', 'N:acc', 'sapiens'],\n",
       " [';', 'SENT', ';'],\n",
       " ['ego', 'PRON', 'ego'],\n",
       " ['autem', 'ADV', 'autem'],\n",
       " ['a', 'PREP', 'a'],\n",
       " ['patre', 'N:abl', 'pater'],\n",
       " ['ita', 'ADV', 'ita'],\n",
       " ['eram', 'ESSE:IND', 'sum'],\n",
       " ['deductus', 'V:PTC:nom', 'deduco'],\n",
       " ['ad', 'PREP', 'ad'],\n",
       " ['Scaevolam', 'NPR', '<unknown>'],\n",
       " ['sumpta', 'V:PTC:nom', 'sumo']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tag(amicitia_words[100:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-6e2cd06a89c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cogito ergo sum\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tt' is not defined"
     ]
    }
   ],
   "source": [
    "tt.tag(\"Cogito ergo sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLTK taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "grk_corpus_importer.import_corpus(\"greek_models_cltk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tag.pos import POSTag\n",
    "tagger = POSTag('greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Œ∏ŒµŒø·Ω∫œÇ', 'N-P---MA-'),\n",
       " ('Œº·Ω≤ŒΩ', 'G--------'),\n",
       " ('Œ±·º∞œÑ·ø∂', 'V1SPIA---'),\n",
       " ('œÑ·ø∂ŒΩŒ¥', 'P-P---MG-'),\n",
       " ('·æΩ', None),\n",
       " ('·ºÄœÄŒ±ŒªŒªŒ±Œ≥·Ω¥ŒΩ', 'N-S---FA-'),\n",
       " ('œÄœåŒΩœâŒΩ', 'N-P---MG-'),\n",
       " ('œÜœÅŒøœÖœÅ·æ∂œÇ', 'N-S---FG-'),\n",
       " ('·ºêœÑŒµŒØŒ±œÇ', 'A-S---FG-'),\n",
       " ('Œº·øÜŒ∫ŒøœÇ', 'N-S---NA-')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_ngram_123_backoff('Œ∏ŒµŒø·Ω∫œÇ Œº·Ω≤ŒΩ Œ±·º∞œÑ·ø∂ œÑ·ø∂ŒΩŒ¥·æΩ ·ºÄœÄŒ±ŒªŒªŒ±Œ≥·Ω¥ŒΩ œÄœåŒΩœâŒΩ œÜœÅŒøœÖœÅ·æ∂œÇ ·ºêœÑŒµŒØŒ±œÇ Œº·øÜŒ∫ŒøœÇ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Œ∏ŒµŒø·Ω∫œÇ', 'N-P---MA-'),\n",
       " ('Œº·Ω≤ŒΩ', 'G--------'),\n",
       " ('Œ±·º∞œÑ·ø∂', 'V1SPIA---'),\n",
       " ('œÑ·ø∂ŒΩŒ¥', 'P-P---NG-'),\n",
       " ('·æΩ', 'Unk'),\n",
       " ('·ºÄœÄŒ±ŒªŒªŒ±Œ≥·Ω¥ŒΩ', 'N-S---FA-'),\n",
       " ('œÄœåŒΩœâŒΩ', 'N-P---MG-'),\n",
       " ('œÜœÅŒøœÖœÅ·æ∂œÇ', 'N-S---FG-'),\n",
       " ('·ºêœÑŒµŒØŒ±œÇ', 'A-S---FG-'),\n",
       " ('Œº·øÜŒ∫ŒøœÇ', 'N-S---NA-')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_tnt('Œ∏ŒµŒø·Ω∫œÇ Œº·Ω≤ŒΩ Œ±·º∞œÑ·ø∂ œÑ·ø∂ŒΩŒ¥·æΩ ·ºÄœÄŒ±ŒªŒªŒ±Œ≥·Ω¥ŒΩ œÄœåŒΩœâŒΩ œÜœÅŒøœÖœÅ·æ∂œÇ ·ºêœÑŒµŒØŒ±œÇ Œº·øÜŒ∫ŒøœÇ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "from cltk.tokenize.word import WordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = WordTokenizer('greek')\n",
    "sentence_tokenizer = TokenizeSentence(\"greek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mat/cltk_data\n",
      "/Users/mat/cltk_data/greek/text/greek_text_perseus/\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschines/aeschin_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschines/aeschin_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschylus/aesch.ag_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschylus/aesch.ag_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschylus/aesch.eum_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschylus/aesch.eum_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschylus/aesch.lib_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschylus/aesch.lib_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschylus/aesch.pb_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschylus/aesch.pb_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschylus/aesch.pers_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschylus/aesch.pers_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschylus/aesch.seven_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschylus/aesch.seven_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschylus/aesch.supp_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aeschylus/aesch.supp_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Andocides/andoc_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Andocides/andoc_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Anth/01_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Anth/01_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Anth/02_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Anth/02_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Anth/03_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Anth/03_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Anth/04_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Anth/04_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Anth/05_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Anth/05_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Apollodorus/apollod_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Apollodorus/apollod_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Apollonius/argo_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Apollonius/argo_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Appian/appian.cw_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Appian/appian.cw_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Appian/appian.fw_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Appian/appian.fw_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aretaeus/aret_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aretaeus/aret_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristides/aristid.orat_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristides/aristid.orat_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristides/aristid.rhet_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristides/aristid.rhet_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.ach_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.ach_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.birds_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.birds_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.cl_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.cl_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.eccl_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.eccl_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.frogs_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.frogs_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.kn_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.kn_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.lys_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.lys_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.peace_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.peace_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.pl_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.pl_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.thes_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.thes_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.wasps_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristophanes/aristoph.wasps_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.ath.pol_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.ath.pol_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.econ_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.econ_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.eud.eth_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.eud.eth_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.met_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.met_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.nic.eth_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.nic.eth_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.pol_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.pol_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.rh_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.rh_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.vir_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Aristotle/aristot.vir_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Arrian/arrian.acies_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Arrian/arrian.acies_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Arrian/arrian.anab_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Arrian/arrian.anab_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Arrian/arrian.cuneg_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Arrian/arrian.cuneg_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Arrian/arrian.indica_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Arrian/arrian.indica_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Arrian/arrian.periplous_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Arrian/arrian.periplous_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Arrian/arrian.tactica_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Arrian/arrian.tactica_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath01_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath01_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath02_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath02_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath03_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath03_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath04_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath04_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath05_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath05_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath06_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath06_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath07_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath07_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath08_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath08_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath09_gk.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath09_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath10_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath10_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath11_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath11_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath12_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath12_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath13_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath13_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath14_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath14_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath15_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Athenaeus/ath15_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Bacchylides/bacchyl_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Bacchylides/bacchyl_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Bible/nt_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Bible/nt_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Colluthus/colluthus.01_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Colluthus/colluthus.01_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Demades/demad_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Demades/demad_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Demosthenes/dem01-10_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Demosthenes/dem01-10_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Demosthenes/dem11-20_gk.txt\n",
      "/Users/mat/cltk_data/greek/text/perseus_unicode/Demosthenes/dem11-20_gk.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-99995bcff38a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mbeta_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbleach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0ma_replacer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplacer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0municode_converted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_replacer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0;31m#print(unicode_converted)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0municode_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcltk_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/greek/text/perseus_unicode/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/cltk/corpus/greek/beta_to_unicode.py\u001b[0m in \u001b[0;36mbeta_code\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0;31m# remove third run, if punct list not used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# my modified version of https://github.com/cltk/greek_text_perseus/blob/master/perseus_compiler.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import bleach\n",
    "#from cltk.corpus.classical_greek.replacer import Replacer\n",
    "from cltk.corpus.greek.beta_to_unicode import Replacer\n",
    "\n",
    "\n",
    "home = os.path.expanduser('~')\n",
    "cltk_path = os.path.join(home, 'cltk_data')\n",
    "print(cltk_path)\n",
    "perseus_root = cltk_path + '/greek/text/greek_text_perseus/'\n",
    "print(perseus_root)\n",
    "ignore = [\n",
    "    '.git',\n",
    "    'LICENSE.md',\n",
    "    'README.md',\n",
    "    'cltk_json',\n",
    "    'json',\n",
    "    'perseus_compiler.py'\n",
    "]\n",
    "authors = [d for d in os.listdir(perseus_root) if d not in ignore]\n",
    "\n",
    "for author in authors:\n",
    "    texts = os.listdir(perseus_root + author + '/opensource')\n",
    "    for text in texts:\n",
    "        text_match = re.match(r'.*_gk.xml', text)\n",
    "        if text_match:\n",
    "            gk_file = text_match.group()\n",
    "            txt_file = perseus_root + author + '/opensource/' + gk_file\n",
    "            with open(txt_file) as gk:\n",
    "                html = gk.read()\n",
    "                beta_code = bleach.clean(html, strip=True).upper()\n",
    "                a_replacer = Replacer()\n",
    "                unicode_converted = a_replacer.beta_code(beta_code)\n",
    "                #print(unicode_converted)\n",
    "                unicode_root = cltk_path + '/greek/text/perseus_unicode/'\n",
    "                unic_pres = os.path.isdir(unicode_root)\n",
    "                if unic_pres is True:\n",
    "                    pass\n",
    "                else:\n",
    "                    os.mkdir(unicode_root)\n",
    "                author_path = unicode_root + author\n",
    "                author_path_pres = os.path.isdir(author_path)\n",
    "                if author_path_pres is True:\n",
    "                    pass\n",
    "                else:\n",
    "                    os.mkdir(author_path)\n",
    "                gk_file_txt = os.path.splitext(gk_file)[0] + '.txt'\n",
    "                uni_write = author_path + '/' + gk_file_txt\n",
    "                print(uni_write)\n",
    "                with open(uni_write, 'w') as uni_write:\n",
    "                    uni_write.write(unicode_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    perseusgreek = PlaintextCorpusReader(\n",
    "        cltk_path + '/greek/text/perseus_unicode/', \n",
    "        '.*\\.txt',\n",
    "        word_tokenizer=word_tokenizer, \n",
    "        sent_tokenizer=sentence_tokenizer, \n",
    "        encoding='utf-8'\n",
    "    )    \n",
    "    pass\n",
    "except IOError as e:\n",
    "    pass\n",
    "    # print(\"Corpus not found. Please check that the Latin Library is installed in CLTK_DATA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds = perseusgreek.words('Aristophanes/aristoph.birds_gk.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['·ºêœÄŒ≠Œ≥ŒµŒπœÅŒøŒΩ', 'Œ±·ΩêœÑœåŒΩ', '.', 'ŒòŒµœÅŒ¨œÄœâŒΩ', '·ºúœÄŒøœÄŒøœÉ', 'Œø·º∂Œ¥Œ±', 'Œº·Ω≤ŒΩ', 'œÉŒ±œÜ·ø∂œÉ', '·ΩÖœÑŒπ', '·ºÄœáŒ∏Œ≠œÉŒµœÑŒ±Œπ', ',', 'œÉœÜ·ø∑ŒΩ', 'Œ¥‚Äô', 'Œ±·ΩêœÑ·Ω∏ŒΩ', 'Œø·ΩïŒΩŒµŒ∫‚Äô', '·ºêœÄŒµŒ≥ŒµœÅ·ø∂', '.', 'Œ†ŒπœÉŒ∏Œ≠œÑŒ±ŒπœÅŒøœÉ', 'Œ∫Œ±Œ∫·ø∂œÇ', 'œÉœç', 'Œ≥‚Äô', '·ºÄœÄœåŒªŒø·øê', ',', '·Ω•œÇ', 'Œº‚Äô', '·ºÄœÄŒ≠Œ∫œÑŒµŒπŒΩŒ±œÇ', 'Œ¥Œ≠ŒµŒπ', '.', '·ºòœÖŒµŒªœÄŒØŒ¥Œ∑œÉ', 'Œø·º¥ŒºŒøŒπ', 'Œ∫Œ±Œ∫ŒøŒ¥Œ±ŒØŒºœâŒΩ', 'œá·Ω†', 'Œ∫ŒøŒªŒøŒπœåœÇ', 'ŒºŒø·º¥œáŒµœÑŒ±Œπ', '·ΩëœÄ·Ω∏', 'œÑŒø·ø¶', 'Œ¥Œ≠ŒøœÖœÇ\\\\', '.', 'Œ†ŒπœÉŒ∏Œ≠œÑŒ±ŒπœÅŒøœÉ', '·Ω¶', 'Œ¥ŒµŒπŒªœåœÑŒ±œÑŒøŒΩ', 'œÉ·Ω∫', 'Œ∏Œ∑œÅŒØŒøŒΩ', ',', 'Œ¥ŒµŒØœÉŒ±œÇ', '·ºÄœÜ·øÜŒ∫Œ±œÇ', 'œÑ·Ω∏ŒΩ', 'Œ∫ŒøŒªŒøŒπœåŒΩ', ';', '·ºòœÖŒµŒªœÄŒØŒ¥Œ∑œÉ', 'Œµ·º∞œÄŒ≠', 'ŒºŒøŒπ', ',', 'œÉ·Ω∫', 'Œ¥·Ω≤', 'œÑ·Ω¥ŒΩ', 'Œ∫ŒøœÅœéŒΩŒ∑ŒΩ', 'Œø·ΩêŒ∫', '·ºÄœÜ·øÜŒ∫Œ±œÇ', 'Œ∫Œ±œÑŒ±œÄŒµœÉœéŒΩ', ';', 'Œ†ŒπœÉŒ∏Œ≠œÑŒ±ŒπœÅŒøœÉ', 'Œº·Ω∞', 'ŒîŒØ‚Äô', 'Œø·ΩêŒ∫', '·ºîŒ≥œâŒ≥Œµ', '.', '·ºòœÖŒµŒªœÄŒØŒ¥Œ∑œÉ', 'œÄŒø·ø¶', 'Œ≥Œ¨œÅ', '·ºêœÉœÑ‚Äô', ';', 'Œ†ŒπœÉŒ∏Œ≠œÑŒ±ŒπœÅŒøœÉ', '·ºÄœÄŒ≠œÄœÑŒµœÑŒø', '.', '·ºòœÖŒµŒªœÄŒØŒ¥Œ∑œÉ', 'Œø·ΩêŒ∫', '·ºÜœÅ‚Äô', '·ºÄœÜ·øÜŒ∫Œ±œÇ', ';', '·Ω¶Œ≥Œ¨Œ∏‚Äô', '·Ω°œÇ', '·ºÄŒΩŒ¥œÅŒµ·øñŒøœÇ', 'Œµ·º∂', '.', '·ºúœÄŒøœà', '·ºÑŒΩŒøŒπŒ≥Œµ', 'œÑ·Ω¥ŒΩ', '·ΩïŒªŒ∑ŒΩ', ',', '·ºµŒΩ‚Äô', '·ºêŒæŒ≠ŒªŒ∏œâ', 'œÄŒøœÑŒ≠', '.', '·ºòœÖŒµŒªœÄŒØŒ¥Œ∑œÉ', '·Ω¶', '·º©œÅŒ¨Œ∫ŒªŒµŒπœÇ', 'œÑŒøœÖœÑ·Ω∂', 'œÑŒØ', 'œÄŒøœÑ‚Äô']\n"
     ]
    }
   ],
   "source": [
    "print(list(birds[1000:1100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('·ºêœÄŒ≠Œ≥ŒµŒπœÅŒøŒΩ', 'Unk'),\n",
       " ('Œ±·ΩêœÑœåŒΩ', 'A-S---MA-'),\n",
       " ('.', 'U--------'),\n",
       " ('ŒòŒµœÅŒ¨œÄœâŒΩ', 'Unk'),\n",
       " ('·ºúœÄŒøœÄŒøœÉ', 'Unk'),\n",
       " ('Œø·º∂Œ¥Œ±', 'V1SRIA---'),\n",
       " ('Œº·Ω≤ŒΩ', 'G--------'),\n",
       " ('œÉŒ±œÜ·ø∂œÉ', 'Unk'),\n",
       " ('·ΩÖœÑŒπ', 'C--------'),\n",
       " ('·ºÄœáŒ∏Œ≠œÉŒµœÑŒ±Œπ', 'Unk'),\n",
       " (',', 'U--------'),\n",
       " ('œÉœÜ·ø∑ŒΩ', 'P-D---MG-'),\n",
       " ('Œ¥', 'G--------'),\n",
       " ('‚Äô', 'Unk'),\n",
       " ('Œ±·ΩêœÑ·Ω∏ŒΩ', 'A-S---MA-'),\n",
       " ('Œø·ΩïŒΩŒµŒ∫', 'C--------'),\n",
       " ('‚Äô', 'Unk'),\n",
       " ('·ºêœÄŒµŒ≥ŒµœÅ·ø∂', 'Unk'),\n",
       " ('.', 'U--------'),\n",
       " ('Œ†ŒπœÉŒ∏Œ≠œÑŒ±ŒπœÅŒøœÉ', 'Unk'),\n",
       " ('Œ∫Œ±Œ∫·ø∂œÇ', 'D--------'),\n",
       " ('œÉœç', 'P-S----N-'),\n",
       " ('Œ≥', 'G--------'),\n",
       " ('‚Äô', 'Unk'),\n",
       " ('·ºÄœÄœåŒªŒø·øê', 'Unk'),\n",
       " (',', 'U--------'),\n",
       " ('·Ω•œÇ', 'C--------'),\n",
       " ('Œº', 'P-S---MA-'),\n",
       " ('‚Äô', 'Unk'),\n",
       " ('·ºÄœÄŒ≠Œ∫œÑŒµŒπŒΩŒ±œÇ', 'Unk'),\n",
       " ('Œ¥Œ≠ŒµŒπ', 'N-S---ND-'),\n",
       " ('.', 'U--------'),\n",
       " ('·ºòœÖŒµŒªœÄŒØŒ¥Œ∑œÉ', 'Unk'),\n",
       " ('Œø·º¥ŒºŒøŒπ', 'E--------'),\n",
       " ('Œ∫Œ±Œ∫ŒøŒ¥Œ±ŒØŒºœâŒΩ', 'Unk'),\n",
       " ('œá·Ω†', 'L-S---MN-'),\n",
       " ('Œ∫ŒøŒªŒøŒπœåœÇ', 'Unk'),\n",
       " ('ŒºŒø·º¥œáŒµœÑŒ±Œπ', 'Unk'),\n",
       " ('·ΩëœÄ·Ω∏', 'R--------'),\n",
       " ('œÑŒø·ø¶', 'L-S---NG-'),\n",
       " ('Œ¥Œ≠ŒøœÖœÇ', 'N-S---NG-'),\n",
       " ('\\\\', 'Unk'),\n",
       " ('.', 'U--------'),\n",
       " ('Œ†ŒπœÉŒ∏Œ≠œÑŒ±ŒπœÅŒøœÉ', 'Unk'),\n",
       " ('·Ω¶', 'E--------'),\n",
       " ('Œ¥ŒµŒπŒªœåœÑŒ±œÑŒøŒΩ', 'Unk'),\n",
       " ('œÉ·Ω∫', 'P-S----N-'),\n",
       " ('Œ∏Œ∑œÅŒØŒøŒΩ', 'N-S---NN-'),\n",
       " (',', 'U--------'),\n",
       " ('Œ¥ŒµŒØœÉŒ±œÇ', 'T-SAPAMN-'),\n",
       " ('·ºÄœÜ·øÜŒ∫Œ±œÇ', 'V2SAIA---'),\n",
       " ('œÑ·Ω∏ŒΩ', 'L-S---MA-'),\n",
       " ('Œ∫ŒøŒªŒøŒπœåŒΩ', 'Unk'),\n",
       " (';', 'U--------'),\n",
       " ('·ºòœÖŒµŒªœÄŒØŒ¥Œ∑œÉ', 'Unk'),\n",
       " ('Œµ·º∞œÄŒ≠', 'V2SAMA---'),\n",
       " ('ŒºŒøŒπ', 'P-S---MD-'),\n",
       " (',', 'U--------'),\n",
       " ('œÉ·Ω∫', 'P-S---MN-'),\n",
       " ('Œ¥·Ω≤', 'G--------'),\n",
       " ('œÑ·Ω¥ŒΩ', 'L-S---FA-'),\n",
       " ('Œ∫ŒøœÅœéŒΩŒ∑ŒΩ', 'N-S---FA-'),\n",
       " ('Œø·ΩêŒ∫', 'D--------'),\n",
       " ('·ºÄœÜ·øÜŒ∫Œ±œÇ', 'V2SAIA---'),\n",
       " ('Œ∫Œ±œÑŒ±œÄŒµœÉœéŒΩ', 'Unk'),\n",
       " (';', 'U--------'),\n",
       " ('Œ†ŒπœÉŒ∏Œ≠œÑŒ±ŒπœÅŒøœÉ', 'Unk'),\n",
       " ('Œº·Ω∞', 'G--------'),\n",
       " ('ŒîŒØ', 'Unk'),\n",
       " ('‚Äô', 'Unk'),\n",
       " ('Œø·ΩêŒ∫', 'D--------'),\n",
       " ('·ºîŒ≥œâŒ≥Œµ', 'P-S---MN-'),\n",
       " ('.', 'U--------'),\n",
       " ('·ºòœÖŒµŒªœÄŒØŒ¥Œ∑œÉ', 'Unk'),\n",
       " ('œÄŒø·ø¶', 'D--------'),\n",
       " ('Œ≥Œ¨œÅ', 'G--------'),\n",
       " ('·ºêœÉœÑ', 'V3SPIA---'),\n",
       " ('‚Äô', 'Unk'),\n",
       " (';', 'U--------'),\n",
       " ('Œ†ŒπœÉŒ∏Œ≠œÑŒ±ŒπœÅŒøœÉ', 'Unk'),\n",
       " ('·ºÄœÄŒ≠œÄœÑŒµœÑŒø', 'Unk'),\n",
       " ('.', 'U--------'),\n",
       " ('·ºòœÖŒµŒªœÄŒØŒ¥Œ∑œÉ', 'Unk'),\n",
       " ('Œø·ΩêŒ∫', 'D--------'),\n",
       " ('·ºÜœÅ', 'D--------'),\n",
       " ('‚Äô', 'Unk'),\n",
       " ('·ºÄœÜ·øÜŒ∫Œ±œÇ', 'V2SAIA---'),\n",
       " (';', 'U--------'),\n",
       " ('·Ω¶Œ≥Œ¨Œ∏', 'Unk'),\n",
       " ('‚Äô', 'Unk'),\n",
       " ('·Ω°œÇ', 'D--------'),\n",
       " ('·ºÄŒΩŒ¥œÅŒµ·øñŒøœÇ', 'Unk'),\n",
       " ('Œµ·º∂', 'V2SPIA---'),\n",
       " ('.', 'U--------'),\n",
       " ('·ºúœÄŒøœà', 'Unk'),\n",
       " ('·ºÑŒΩŒøŒπŒ≥Œµ', 'V2SPMA---'),\n",
       " ('œÑ·Ω¥ŒΩ', 'L-S---FA-'),\n",
       " ('·ΩïŒªŒ∑ŒΩ', 'N-S---FA-'),\n",
       " (',', 'U--------'),\n",
       " ('·ºµŒΩ', 'C--------'),\n",
       " ('‚Äô', 'Unk'),\n",
       " ('·ºêŒæŒ≠ŒªŒ∏œâ', 'Unk'),\n",
       " ('œÄŒøœÑŒ≠', 'G--------'),\n",
       " ('.', 'U--------'),\n",
       " ('·ºòœÖŒµŒªœÄŒØŒ¥Œ∑œÉ', 'Unk'),\n",
       " ('·Ω¶', 'E--------'),\n",
       " ('·º©œÅŒ¨Œ∫ŒªŒµŒπœÇ', 'Unk'),\n",
       " ('œÑŒøœÖœÑ·Ω∂', 'Unk'),\n",
       " ('œÑŒØ', 'P-S---NA-'),\n",
       " ('œÄŒøœÑ', 'G--------'),\n",
       " ('‚Äô', 'Unk')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_tnt(\" \".join(birds[1000:1100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TreeTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "### Latin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyCollatinus\n",
    "\n",
    "* Python port of the [Collatinus lemmatizer](https://github.com/biblissima/collatinus)\n",
    "* good if you can read some French (or at least practice it) üòâ\n",
    "* the PoS tags used by Collatinus are explained [here](https://github.com/biblissima/collatinus/blob/master/NOTES_Tagger.md)\n",
    "* morphological analysis not readily machine readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import and instantiate the PyCollatinus lemmatizer (`Lemmatiseur`) ‚Äì and ignore the long list of warnings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: honor has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: aer has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: tethys has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: opes has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: dos has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: corpus has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: ciuis has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: thales has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: poesis has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: manes has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: turris has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: uis has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: nauis has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: apis has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: mare has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: moenia has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: animal has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: mille has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: ille has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n"
     ]
    }
   ],
   "source": [
    "from pycollatinus import Lemmatiseur\n",
    "analyzer = Lemmatiseur()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmatiser can take as input a **single word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'desinence': 'ito',\n",
       "  'form': 'cogito',\n",
       "  'lemma': 'cogo',\n",
       "  'morph': '2√®me singulier imp√©ratif futur actif',\n",
       "  'radical': 'cog'},\n",
       " {'desinence': 'ito',\n",
       "  'form': 'cogito',\n",
       "  'lemma': 'cogo',\n",
       "  'morph': '3√®me singulier imp√©ratif futur actif',\n",
       "  'radical': 'cog'},\n",
       " {'desinence': 'o',\n",
       "  'form': 'cogito',\n",
       "  'lemma': 'cogito',\n",
       "  'morph': '1√®re singulier indicatif pr√©sent actif',\n",
       "  'radical': 'cogit'},\n",
       " {'desinence': 'o',\n",
       "  'form': 'cogito',\n",
       "  'lemma': 'cogito',\n",
       "  'morph': '1√®re singulier indicatif pr√©sent actif',\n",
       "  'radical': 'cogit'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(analyzer.lemmatise(\"Cogito\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or an **entire sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'desinence': 'ito',\n",
       "   'form': 'cogito',\n",
       "   'lemma': 'cogo',\n",
       "   'morph': '2√®me singulier imp√©ratif futur actif',\n",
       "   'radical': 'cog'},\n",
       "  {'desinence': 'ito',\n",
       "   'form': 'cogito',\n",
       "   'lemma': 'cogo',\n",
       "   'morph': '3√®me singulier imp√©ratif futur actif',\n",
       "   'radical': 'cog'},\n",
       "  {'desinence': 'o',\n",
       "   'form': 'cogito',\n",
       "   'lemma': 'cogito',\n",
       "   'morph': '1√®re singulier indicatif pr√©sent actif',\n",
       "   'radical': 'cogit'},\n",
       "  {'desinence': 'o',\n",
       "   'form': 'cogito',\n",
       "   'lemma': 'cogito',\n",
       "   'morph': '1√®re singulier indicatif pr√©sent actif',\n",
       "   'radical': 'cogit'}],\n",
       " [{'desinence': 'o',\n",
       "   'form': 'ergo',\n",
       "   'lemma': 'ergo',\n",
       "   'morph': '1√®re singulier indicatif pr√©sent actif',\n",
       "   'radical': 'erg'},\n",
       "  {'desinence': '',\n",
       "   'form': 'ergo',\n",
       "   'lemma': 'ergo',\n",
       "   'morph': '-',\n",
       "   'radical': 'ergo'},\n",
       "  {'desinence': '',\n",
       "   'form': 'ergo',\n",
       "   'lemma': 'ergo',\n",
       "   'morph': 'positif',\n",
       "   'radical': 'ergo'}],\n",
       " [{'desinence': 'um',\n",
       "   'form': 'sum',\n",
       "   'lemma': 'sum',\n",
       "   'morph': '1√®re singulier indicatif pr√©sent actif',\n",
       "   'radical': 's'}]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(analyzer.lemmatise_multiple(\"Cogito ergo sum\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to output the lemmatisation in a more intellegible way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\tcogito\t\tcogo 2√®me singulier imp√©ratif futur actif\n",
      "1.2\tcogito\t\tcogo 3√®me singulier imp√©ratif futur actif\n",
      "1.3\tcogito\t\tcogito 1√®re singulier indicatif pr√©sent actif\n",
      "1.4\tcogito\t\tcogito 1√®re singulier indicatif pr√©sent actif\n",
      "2.1\tergo\t\tergo 1√®re singulier indicatif pr√©sent actif\n",
      "2.2\tergo\t\tergo -\n",
      "2.3\tergo\t\tergo positif\n",
      "3.1\tsum\t\tsum 1√®re singulier indicatif pr√©sent actif\n"
     ]
    }
   ],
   "source": [
    "# the analyzer output is essentially a list of lists\n",
    "# for each analyzed token it returns a list of possible lemmata\n",
    "# here we iterate through both lists and display the analysis as we go along\n",
    "\n",
    "for n, result in enumerate(analyzer.lemmatise_multiple(\"Cogito ergo sum\")):\n",
    "    for i, lemma in enumerate(result):\n",
    "        print(\n",
    "            \"{}.{}\\t{}\\t\\t{} {}\".format(\n",
    "                n + 1,\n",
    "                i + 1,\n",
    "                lemma[\"form\"],\n",
    "                lemma[\"lemma\"],\n",
    "                lemma[\"morph\"]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same but also with PoS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\tcogito\tv\tcogo 2√®me singulier imp√©ratif futur actif\n",
      "1.2\tcogito\tv\tcogo 3√®me singulier imp√©ratif futur actif\n",
      "1.3\tcogito\tv\tcogito 1√®re singulier indicatif pr√©sent actif\n",
      "1.4\tcogito\tv\tcogito 1√®re singulier indicatif pr√©sent actif\n",
      "2.1\tergo\tv\tergo 1√®re singulier indicatif pr√©sent actif\n",
      "2.2\tergo\tc\tergo -\n",
      "2.3\tergo\td\tergo positif\n",
      "3.1\tsum\tv\tsum 1√®re singulier indicatif pr√©sent actif\n"
     ]
    }
   ],
   "source": [
    "# the analyzer output is essentially a list of lists\n",
    "# for each analyzed token it returns a list of possible lemmata\n",
    "# here we iterate through both lists and display the analysis as we go along\n",
    "\n",
    "for n, result in enumerate(analyzer.lemmatise_multiple(\"Cogito ergo sum\", pos=True),):\n",
    "    for i, lemma in enumerate(result):\n",
    "        print(\n",
    "            \"{}.{}\\t{}\\t{}\\t{} {}\".format(\n",
    "                n + 1,\n",
    "                i + 1,\n",
    "                lemma[\"form\"],\n",
    "                lemma[\"pos\"],\n",
    "                lemma[\"lemma\"],\n",
    "                lemma[\"morph\"]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "234px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
