{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech tagging and lemmatisation with 🐍\n",
    "\n",
    "See information on the [Sunoikisis Wiki](https://github.com/SunoikisisDC/SunoikisisDC-2017-2018/wiki/Python-2:-Part-of-Speech-tagging-and-lemmatisation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "* present tools/libraries that can be used for PoS tagging and lemmatisation\n",
    "* raise awareness about what \"goes on\" behind the scenes, and that libraries make transparent to the user\n",
    "* show that there is a growing amount of python code/libraries for linguistic annotation on Anc Greek/Latin\n",
    "* ... but same time it takes a bit of bricolage to get things to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import cltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.83'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cltk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grk_corpus_importer = CorpusImporter('greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greek_software_tlgu',\n",
       " 'greek_text_perseus',\n",
       " 'phi7',\n",
       " 'tlg',\n",
       " 'greek_proper_names_cltk',\n",
       " 'greek_models_cltk',\n",
       " 'greek_treebank_perseus',\n",
       " 'greek_lexica_perseus',\n",
       " 'greek_training_set_sentence_cltk',\n",
       " 'greek_word2vec_cltk',\n",
       " 'greek_text_lacus_curtius',\n",
       " 'greek_text_first1kgreek']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grk_corpus_importer.list_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 100% 143.79 MiB | 4.02 MiB/s s \r"
     ]
    }
   ],
   "source": [
    "grk_corpus_importer.import_corpus('greek_text_perseus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.latin import latinlibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_corpus_importer = CorpusImporter('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latin_text_perseus',\n",
       " 'latin_treebank_perseus',\n",
       " 'latin_text_latin_library',\n",
       " 'phi5',\n",
       " 'phi7',\n",
       " 'latin_proper_names_cltk',\n",
       " 'latin_models_cltk',\n",
       " 'latin_pos_lemmata_cltk',\n",
       " 'latin_treebank_index_thomisticus',\n",
       " 'latin_lexica_perseus',\n",
       " 'latin_training_set_sentence_cltk',\n",
       " 'latin_word2vec_cltk',\n",
       " 'latin_text_antique_digiliblt',\n",
       " 'latin_text_corpus_grammaticorum_latinorum',\n",
       " 'latin_text_poeti_ditalia']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_corpus_importer.list_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_corpus_importer.import_corpus('latin_training_set_sentence_cltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 100% 35.50 MiB | 5.91 MiB/s \r"
     ]
    }
   ],
   "source": [
    "la_corpus_importer.import_corpus('latin_text_latin_library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.latin import latinlibrary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `cltk.corpus.latin.latinlibrary` is a shortcut for several things, and there is nothing comparable (yet) for Greek (see [source code](https://github.com/cltk/cltk/blob/master/cltk/corpus/latin/__init__.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "amicitia_words = latinlibrary.words('cicero/amic.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11618"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amicitia_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get `n` number of tokens from this text by using the *slice notation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cicero',\n",
       " ':',\n",
       " 'de',\n",
       " 'Amicitia',\n",
       " 'M.',\n",
       " 'TVLLI',\n",
       " 'CICERONIS',\n",
       " 'LAELIVS',\n",
       " 'DE',\n",
       " 'AMICITIA']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first ten tokens\n",
    "amicitia_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or the last token\n",
    "amicitia_words[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also count occurrences by using the `count()` method and passing as parameter the token we want to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amicitia_words.count('et')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amicitia_words.count('amicitia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look to the `type` of the variable `amicitia_words` where we loaded the content of Cicero's *De Amicitia*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.util.StreamBackedCorpusView"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(amicitia_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on StreamBackedCorpusView in module nltk.corpus.reader.util object:\n",
      "\n",
      "class StreamBackedCorpusView(nltk.collections.AbstractLazySequence)\n",
      " |  A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      " |  it can be accessed by index, iterated over, etc.  However, the\n",
      " |  tokens are only constructed as-needed -- the entire corpus is\n",
      " |  never stored in memory at once.\n",
      " |  \n",
      " |  The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      " |  a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      " |  and a block reader.  A \"block reader\" is a function that reads\n",
      " |  zero or more tokens from a stream, and returns them as a list.  A\n",
      " |  very simple example of a block reader is:\n",
      " |  \n",
      " |      >>> def simple_block_reader(stream):\n",
      " |      ...     return stream.readline().split()\n",
      " |  \n",
      " |  This simple block reader reads a single line at a time, and\n",
      " |  returns a single token (consisting of a string) for each\n",
      " |  whitespace-separated substring on the line.\n",
      " |  \n",
      " |  When deciding how to define the block reader for a given\n",
      " |  corpus, careful consideration should be given to the size of\n",
      " |  blocks handled by the block reader.  Smaller block sizes will\n",
      " |  increase the memory requirements of the corpus view's internal\n",
      " |  data structures (by 2 integers per block).  On the other hand,\n",
      " |  larger block sizes may decrease performance for random access to\n",
      " |  the corpus.  (But note that larger block sizes will *not*\n",
      " |  decrease performance for iteration.)\n",
      " |  \n",
      " |  Internally, ``CorpusView`` maintains a partial mapping from token\n",
      " |  index to file position, with one entry per block.  When a token\n",
      " |  with a given index *i* is requested, the ``CorpusView`` constructs\n",
      " |  it as follows:\n",
      " |  \n",
      " |    1. First, it searches the toknum/filepos mapping for the token\n",
      " |       index closest to (but less than or equal to) *i*.\n",
      " |  \n",
      " |    2. Then, starting at the file position corresponding to that\n",
      " |       index, it reads one block at a time using the block reader\n",
      " |       until it reaches the requested token.\n",
      " |  \n",
      " |  The toknum/filepos mapping is created lazily: it is initially\n",
      " |  empty, but every time a new block is read, the block's\n",
      " |  initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      " |  map has one entry per block.)\n",
      " |  \n",
      " |  In order to increase efficiency for random access patterns that\n",
      " |  have high degrees of locality, the corpus view may cache one or\n",
      " |  more blocks.\n",
      " |  \n",
      " |  :note: Each ``CorpusView`` object internally maintains an open file\n",
      " |      object for its underlying corpus file.  This file should be\n",
      " |      automatically closed when the ``CorpusView`` is garbage collected,\n",
      " |      but if you wish to close it manually, use the ``close()``\n",
      " |      method.  If you access a ``CorpusView``'s items after it has been\n",
      " |      closed, the file object will be automatically re-opened.\n",
      " |  \n",
      " |  :warning: If the contents of the file are modified during the\n",
      " |      lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      " |      is undefined.\n",
      " |  \n",
      " |  :warning: If a unicode encoding is specified when constructing a\n",
      " |      ``CorpusView``, then the block reader may only call\n",
      " |      ``stream.seek()`` with offsets that have been returned by\n",
      " |      ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      " |      relative offsets, or with offsets based on string lengths, may\n",
      " |      lead to incorrect behavior.\n",
      " |  \n",
      " |  :ivar _block_reader: The function used to read\n",
      " |      a single block from the underlying file stream.\n",
      " |  :ivar _toknum: A list containing the token index of each block\n",
      " |      that has been processed.  In particular, ``_toknum[i]`` is the\n",
      " |      token index of the first token in block ``i``.  Together\n",
      " |      with ``_filepos``, this forms a partial mapping between token\n",
      " |      indices and file positions.\n",
      " |  :ivar _filepos: A list containing the file position of each block\n",
      " |      that has been processed.  In particular, ``_toknum[i]`` is the\n",
      " |      file position of the first character in block ``i``.  Together\n",
      " |      with ``_toknum``, this forms a partial mapping between token\n",
      " |      indices and file positions.\n",
      " |  :ivar _stream: The stream used to access the underlying corpus file.\n",
      " |  :ivar _len: The total number of tokens in the corpus, if known;\n",
      " |      or None, if the number of tokens is not yet known.\n",
      " |  :ivar _eofpos: The character position of the last character in the\n",
      " |      file.  This is calculated when the corpus view is initialized,\n",
      " |      and is used to decide when the end of file has been reached.\n",
      " |  :ivar _cache: A cache of the most recently read block.  It\n",
      " |     is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      " |     start_toknum is the token index of the first token in the block;\n",
      " |     end_toknum is the token index of the first token not in the\n",
      " |     block; and tokens is a list of the tokens in the block.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StreamBackedCorpusView\n",
      " |      nltk.collections.AbstractLazySequence\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Return a list concatenating self with other.\n",
      " |  \n",
      " |  __getitem__(self, i)\n",
      " |      Return the *i* th token in the corpus file underlying this\n",
      " |      corpus view.  Negative indices and spans are both supported.\n",
      " |  \n",
      " |  __init__(self, fileid, block_reader=None, startpos=0, encoding='utf8')\n",
      " |      Create a new corpus view, based on the file ``fileid``, and\n",
      " |      read with ``block_reader``.  See the class documentation\n",
      " |      for more information.\n",
      " |      \n",
      " |      :param fileid: The path to the file that is read by this\n",
      " |          corpus view.  ``fileid`` can either be a string or a\n",
      " |          ``PathPointer``.\n",
      " |      \n",
      " |      :param startpos: The file position at which the view will\n",
      " |          start reading.  This can be used to skip over preface\n",
      " |          sections.\n",
      " |      \n",
      " |      :param encoding: The unicode encoding that should be used to\n",
      " |          read the file's contents.  If no encoding is specified,\n",
      " |          then the file's contents will be read as a non-unicode\n",
      " |          string (i.e., a str).\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of tokens in the corpus file underlying this\n",
      " |      corpus view.\n",
      " |  \n",
      " |  __mul__(self, count)\n",
      " |      Return a list concatenating self with itself ``count`` times.\n",
      " |  \n",
      " |  __radd__(self, other)\n",
      " |      Return a list concatenating other with self.\n",
      " |  \n",
      " |  __rmul__(self, count)\n",
      " |      Return a list concatenating self with itself ``count`` times.\n",
      " |  \n",
      " |  close(self)\n",
      " |      Close the file stream associated with this corpus view.  This\n",
      " |      can be useful if you are worried about running out of file\n",
      " |      handles (although the stream should automatically be closed\n",
      " |      upon garbage collection of the corpus view).  If the corpus\n",
      " |      view is accessed after it is closed, it will be automatically\n",
      " |      re-opened.\n",
      " |  \n",
      " |  iterate_from(self, start_tok)\n",
      " |      Return an iterator that generates the tokens in the corpus\n",
      " |      file underlying this corpus view, starting at the token number\n",
      " |      ``start``.  If ``start>=len(self)``, then this iterator will\n",
      " |      generate no tokens.\n",
      " |  \n",
      " |  read_block(self, stream)\n",
      " |      Read a block from the input stream.\n",
      " |      \n",
      " |      :return: a block of tokens from the input stream\n",
      " |      :rtype: list(any)\n",
      " |      :param stream: an input stream\n",
      " |      :type stream: stream\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  fileid\n",
      " |      The fileid of the file that is accessed by this view.\n",
      " |      \n",
      " |      :type: str or PathPointer\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.collections.AbstractLazySequence:\n",
      " |  \n",
      " |  __contains__(self, value)\n",
      " |      Return true if this list contains ``value``.\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, other, NotImplemented=NotImplemented)\n",
      " |      Return a >= b.  Computed by @total_ordering from (not a < b).\n",
      " |  \n",
      " |  __gt__(self, other, NotImplemented=NotImplemented)\n",
      " |      Return a > b.  Computed by @total_ordering from (not a < b) and (a != b).\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      :raise ValueError: Corpus view objects are unhashable.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return an iterator that generates the tokens in the corpus\n",
      " |      file underlying this corpus view.\n",
      " |  \n",
      " |  __le__(self, other, NotImplemented=NotImplemented)\n",
      " |      Return a <= b.  Computed by @total_ordering from (a < b) or (a == b).\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return a string representation for this corpus view that is\n",
      " |      similar to a list's representation; but if it would be more\n",
      " |      than 60 characters long, it is truncated.\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  count(self, value)\n",
      " |      Return the number of times this list contains ``value``.\n",
      " |  \n",
      " |  index(self, value, start=None, stop=None)\n",
      " |      Return the index of the first occurrence of ``value`` in this\n",
      " |      list that is greater than or equal to ``start`` and less than\n",
      " |      ``stop``.  Negative start and stop values are treated like negative\n",
      " |      slice bounds -- i.e., they count from the end of the list.\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |      Return a string representation for this corpus view that is\n",
      " |      similar to a list's representation; but if it would be more\n",
      " |      than 60 characters long, it is truncated.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.collections.AbstractLazySequence:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(amicitia_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latin\n",
    "\n",
    "#### CLTK taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Gallia', None),\n",
       " ('est', 'V3SPIA---'),\n",
       " ('omnis', 'A-S---MN-'),\n",
       " ('divisa', 'T-PRPPNN-'),\n",
       " ('in', 'R--------'),\n",
       " ('partes', 'N-P---FA-'),\n",
       " ('tres', 'M--------')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cltk.tag.pos import POSTag\n",
    "tagger = POSTag('latin')\n",
    "tagger.tag_ngram_123_backoff('Gallia est omnis divisa in partes tres')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is it failing?**\n",
    "\n",
    "CLTK relies on some components (e.g. trained models) that are read from disk instead of being generated on the fly.\n",
    "\n",
    "This is very common, especially in those cases where the time needed to generate certain objects is not negligible.\n",
    "\n",
    "`pickle` is the python library that does this, and *serialization* is the process of writing an object to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latin_text_perseus',\n",
       " 'latin_treebank_perseus',\n",
       " 'latin_text_latin_library',\n",
       " 'phi5',\n",
       " 'phi7',\n",
       " 'latin_proper_names_cltk',\n",
       " 'latin_models_cltk',\n",
       " 'latin_pos_lemmata_cltk',\n",
       " 'latin_treebank_index_thomisticus',\n",
       " 'latin_lexica_perseus',\n",
       " 'latin_training_set_sentence_cltk',\n",
       " 'latin_word2vec_cltk',\n",
       " 'latin_text_antique_digiliblt',\n",
       " 'latin_text_corpus_grammaticorum_latinorum',\n",
       " 'latin_text_poeti_ditalia']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_corpus_importer.list_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_corpus_importer.import_corpus('latin_models_cltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = POSTag('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cltk.tag.pos.POSTag at 0x10f5bd7f0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('91', 'Unk'), ('91', None), ('91', 'A-P---FN-')),\n",
       " (('92', 'Unk'), ('92', None), ('92', 'N-P---FN-')),\n",
       " (('93', 'Unk'), ('93', None), ('93', 'A-P---FN-')),\n",
       " (('94', 'Unk'), ('94', None), ('94', 'N-P---FN-')),\n",
       " (('95', 'Unk'), ('95', None), ('95', 'A-P---FN-')),\n",
       " (('96', 'Unk'), ('96', None), ('96', 'N-P---FN-')),\n",
       " (('97', 'Unk'), ('97', None), ('97', 'A-P---FN-')),\n",
       " (('98', 'Unk'), ('98', None), ('98', 'N-P---FN-')),\n",
       " (('99', 'Unk'), ('99', None), ('99', 'A-P---FN-')),\n",
       " (('100', 'Unk'), ('100', None), ('100', 'N-P---FN-')),\n",
       " (('101', 'Unk'), ('101', None), ('101', 'A-P---FN-')),\n",
       " (('102', 'Unk'), ('102', None), ('102', 'N-P---FN-')),\n",
       " (('103', 'Unk'), ('103', None), ('103', 'A-P---FN-')),\n",
       " (('104', 'Unk'), ('104', None), ('104', 'N-P---FN-')),\n",
       " (('[', 'U--------'), ('[', 'U--------'), ('[', 'U--------')),\n",
       " (('1', 'Unk'), ('1', None), ('1', 'N-S---MV-')),\n",
       " ((']', 'U--------'), (']', 'U--------'), (']', 'U--------')),\n",
       " (('Q', 'Unk'), ('Q', None), ('Q', 'N-S---MV-')),\n",
       " (('.', 'U--------'), ('.', 'U--------'), ('.', 'U--------')),\n",
       " (('Mucius', 'Unk'), ('Mucius', None), ('Mucius', 'D--------')),\n",
       " (('augur', 'Unk'), ('augur', None), ('augur', 'D--------')),\n",
       " (('multa', 'A-P---NA-'), ('multa', 'A-P---NA-'), ('multa', 'A-P---NA-')),\n",
       " (('narrare', 'V--PNA---'),\n",
       "  ('narrare', 'V--PNA---'),\n",
       "  ('narrare', 'V--PNA---')),\n",
       " (('de', 'R--------'), ('de', 'R--------'), ('de', 'R--------')),\n",
       " (('C', 'Unk'), ('C', None), ('C', '---------')),\n",
       " (('.', 'U--------'), ('.', 'U--------'), ('.', 'U--------')),\n",
       " (('Laelio', 'Unk'), ('Laelio', None), ('Laelio', 'A-S---NB-')),\n",
       " (('socero', 'Unk'), ('socero', None), ('socero', 'N-S---NB-')),\n",
       " (('suo', 'A-S---NB-'), ('suo', 'A-S---NB-'), ('suo', 'A-S---NB-')),\n",
       " (('memoriter', 'Unk'), ('memoriter', None), ('memoriter', 'D--------')),\n",
       " (('et', 'C--------'), ('et', 'C--------'), ('et', 'C--------')),\n",
       " (('iucunde', 'Unk'), ('iucunde', None), ('iucunde', 'D--------')),\n",
       " (('solebat', 'V3SIIA---'),\n",
       "  ('solebat', 'V3SIIA---'),\n",
       "  ('solebat', 'V3SIIA---')),\n",
       " (('nec', 'C--------'), ('nec', 'C--------'), ('nec', 'C--------')),\n",
       " (('dubitare', 'Unk'), ('dubitare', None), ('dubitare', 'V--PNA---')),\n",
       " (('illum', 'P-S---MA-'), ('illum', 'P-S---MA-'), ('illum', 'P-S---MA-')),\n",
       " (('in', 'R--------'), ('in', 'R--------'), ('in', 'R--------')),\n",
       " (('omni', 'A-S---MB-'), ('omni', 'A-S---FB-'), ('omni', 'A-S---FB-')),\n",
       " (('sermone', 'N-S---MB-'),\n",
       "  ('sermone', 'N-S---MB-'),\n",
       "  ('sermone', 'N-S---FB-')),\n",
       " (('appellare', 'V--PNA---'),\n",
       "  ('appellare', 'V--PNA---'),\n",
       "  ('appellare', 'V--PNA---')),\n",
       " (('sapientem', 'Unk'), ('sapientem', None), ('sapientem', 'N-S---FA-')),\n",
       " ((';', 'Unk'), (';', None), (';', 'U--------')),\n",
       " (('ego', 'P-S---MN-'), ('ego', 'P-S---MN-'), ('ego', 'P-S---MN-')),\n",
       " (('autem', 'C--------'), ('autem', 'C--------'), ('autem', 'C--------')),\n",
       " (('a', 'R--------'), ('a', 'R--------'), ('a', 'R--------')),\n",
       " (('patre', 'N-S---MB-'), ('patre', 'N-S---MB-'), ('patre', 'N-S---MB-')),\n",
       " (('ita', 'D--------'), ('ita', 'D--------'), ('ita', 'D--------')),\n",
       " (('eram', 'V1SIIA---'), ('eram', 'V1SIIA---'), ('eram', 'N-S---FA-')),\n",
       " (('deductus', 'T-SRPPMN-'),\n",
       "  ('deductus', 'T-SRPPMN-'),\n",
       "  ('deductus', 'T-SRPPMN-')),\n",
       " (('ad', 'R--------'), ('ad', 'R--------'), ('ad', 'R--------')),\n",
       " (('Scaevolam', 'Unk'), ('Scaevolam', None), ('Scaevolam', 'N-S---FA-')),\n",
       " (('sumpta', 'Unk'), ('sumpta', None), ('sumpta', 'T-SRPPFN-'))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(\n",
    "    tagger.tag_tnt(\" \".join([str(w) for w in amicitia_words[100:150]])),\n",
    "    tagger.tag_ngram_123_backoff(\" \".join([str(w) for w in amicitia_words[100:150]])),\n",
    "    tagger.tag_crf(\" \".join([str(w) for w in amicitia_words[100:150]]))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to take sentences instead of ranges of tokens?\n",
    "\n",
    "The class `PlaintextCorpusReader` has a nice method – `sents()` – that does this.\n",
    "\n",
    "Let's see how it works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_amicitia_sentences = latinlibrary.sents('cicero/amic.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " '4',\n",
       " ']',\n",
       " 'Cum',\n",
       " 'enim',\n",
       " 'saepe',\n",
       " 'cum',\n",
       " 'me',\n",
       " 'ageres',\n",
       " 'ut',\n",
       " 'de',\n",
       " 'amicitia',\n",
       " 'scriberem',\n",
       " 'aliquid',\n",
       " ',',\n",
       " 'digna',\n",
       " 'mihi',\n",
       " 'res',\n",
       " 'cum',\n",
       " 'omnium',\n",
       " 'cognitione',\n",
       " 'tum',\n",
       " 'nostra',\n",
       " 'familiaritate',\n",
       " 'visa',\n",
       " 'est',\n",
       " '.']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_amicitia_sentences[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making sense of PoS tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/francescomambrini/gAGDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.expanduser('~/Documents/gAGDT/'))\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treebanks import Morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pos_tag = 'T-SRPPMN-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-33b0c63b1b73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMorph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_pos_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/gAGDT/treebanks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tag: {} is invalid\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'T'"
     ]
    }
   ],
   "source": [
    "Morph(my_pos_tag).full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'case': 'nominative',\n",
       " 'degree': '-',\n",
       " 'gender': 'masculine',\n",
       " 'mood': 'participle',\n",
       " 'number': 'singular',\n",
       " 'person': '-',\n",
       " 'pos': 'verb',\n",
       " 'tense': 'perfect',\n",
       " 'voice': 'passive'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Morph(my_pos_tag.lower()).full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnt_output = tagger.tag_tnt(\" \".join([str(w) for w in amicitia_words[100:150]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('91', 'Unk'),\n",
       " ('92', 'Unk'),\n",
       " ('93', 'Unk'),\n",
       " ('94', 'Unk'),\n",
       " ('95', 'Unk'),\n",
       " ('96', 'Unk'),\n",
       " ('97', 'Unk'),\n",
       " ('98', 'Unk'),\n",
       " ('99', 'Unk'),\n",
       " ('100', 'Unk'),\n",
       " ('101', 'Unk'),\n",
       " ('102', 'Unk'),\n",
       " ('103', 'Unk'),\n",
       " ('104', 'Unk'),\n",
       " ('[', 'U--------'),\n",
       " ('1', 'Unk'),\n",
       " (']', 'U--------'),\n",
       " ('Q', 'Unk'),\n",
       " ('.', 'U--------'),\n",
       " ('Mucius', 'Unk'),\n",
       " ('augur', 'Unk'),\n",
       " ('multa', 'A-P---NA-'),\n",
       " ('narrare', 'V--PNA---'),\n",
       " ('de', 'R--------'),\n",
       " ('C', 'Unk'),\n",
       " ('.', 'U--------'),\n",
       " ('Laelio', 'Unk'),\n",
       " ('socero', 'Unk'),\n",
       " ('suo', 'A-S---NB-'),\n",
       " ('memoriter', 'Unk'),\n",
       " ('et', 'C--------'),\n",
       " ('iucunde', 'Unk'),\n",
       " ('solebat', 'V3SIIA---'),\n",
       " ('nec', 'C--------'),\n",
       " ('dubitare', 'Unk'),\n",
       " ('illum', 'P-S---MA-'),\n",
       " ('in', 'R--------'),\n",
       " ('omni', 'A-S---MB-'),\n",
       " ('sermone', 'N-S---MB-'),\n",
       " ('appellare', 'V--PNA---'),\n",
       " ('sapientem', 'Unk'),\n",
       " (';', 'Unk'),\n",
       " ('ego', 'P-S---MN-'),\n",
       " ('autem', 'C--------'),\n",
       " ('a', 'R--------'),\n",
       " ('patre', 'N-S---MB-'),\n",
       " ('ita', 'D--------'),\n",
       " ('eram', 'V1SIIA---'),\n",
       " ('deductus', 'T-SRPPMN-'),\n",
       " ('ad', 'R--------'),\n",
       " ('Scaevolam', 'Unk'),\n",
       " ('sumpta', 'Unk')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnt_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unk\n",
      "Unk\n",
      "Unk\n",
      "Unk\n",
      "Unk\n",
      "Unk\n",
      "Unk\n",
      "Unk\n",
      "Unk\n",
      "Unk\n",
      "Unk\n",
      "Unk\n",
      "Unk\n",
      "Unk\n",
      "U--------\n",
      "{'pos': 'punctuation', 'person': '-', 'number': '-', 'tense': '-', 'mood': '-', 'voice': '-', 'gender': '-', 'case': '-', 'degree': '-'}\n",
      "Unk\n",
      "U--------\n",
      "{'pos': 'punctuation', 'person': '-', 'number': '-', 'tense': '-', 'mood': '-', 'voice': '-', 'gender': '-', 'case': '-', 'degree': '-'}\n",
      "Unk\n",
      "U--------\n",
      "{'pos': 'punctuation', 'person': '-', 'number': '-', 'tense': '-', 'mood': '-', 'voice': '-', 'gender': '-', 'case': '-', 'degree': '-'}\n",
      "Unk\n",
      "Unk\n",
      "A-P---NA-\n",
      "{'pos': 'adjective', 'person': '-', 'number': 'plural', 'tense': '-', 'mood': '-', 'voice': '-', 'gender': 'neuter', 'case': 'accusative', 'degree': '-'}\n",
      "V--PNA---\n",
      "{'pos': 'verb', 'person': '-', 'number': '-', 'tense': 'present', 'mood': 'infinitive', 'voice': 'active', 'gender': '-', 'case': '-', 'degree': '-'}\n",
      "R--------\n",
      "{'pos': 'preposition', 'person': '-', 'number': '-', 'tense': '-', 'mood': '-', 'voice': '-', 'gender': '-', 'case': '-', 'degree': '-'}\n",
      "Unk\n",
      "U--------\n",
      "{'pos': 'punctuation', 'person': '-', 'number': '-', 'tense': '-', 'mood': '-', 'voice': '-', 'gender': '-', 'case': '-', 'degree': '-'}\n",
      "Unk\n",
      "Unk\n",
      "A-S---NB-\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-4f63d37acb21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpos_tag\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"Unk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMorph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/gAGDT/treebanks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'b'"
     ]
    }
   ],
   "source": [
    "for token, pos_tag in tnt_output:\n",
    "    print(pos_tag)\n",
    "    if pos_tag != \"Unk\":\n",
    "        print(Morph(pos_tag.lower()).full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \t punctuation\n",
      "\n",
      "] \t punctuation\n",
      "\n",
      ". \t punctuation\n",
      "\n",
      "multa \t adjective\n",
      "\n",
      "narrare \t verb\n",
      "\n",
      "de \t preposition\n",
      "\n",
      ". \t punctuation\n",
      "\n",
      "Expanded form for A-S---NB- not available (error: 'b')\n",
      "et \t conjunction\n",
      "\n",
      "solebat \t verb\n",
      "\n",
      "nec \t conjunction\n",
      "\n",
      "illum \t pron\n",
      "\n",
      "in \t preposition\n",
      "\n",
      "Expanded form for A-S---MB- not available (error: 'b')\n",
      "Expanded form for N-S---MB- not available (error: 'b')\n",
      "appellare \t verb\n",
      "\n",
      "ego \t pron\n",
      "\n",
      "autem \t conjunction\n",
      "\n",
      "a \t preposition\n",
      "\n",
      "Expanded form for N-S---MB- not available (error: 'b')\n",
      "ita \t adverb\n",
      "\n",
      "eram \t verb\n",
      "\n",
      "deductus \t verb\n",
      "\n",
      "ad \t preposition\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token, pos_tag in tnt_output:\n",
    "    if pos_tag != \"Unk\":\n",
    "        try:\n",
    "            pos_info = Morph(pos_tag.lower()).full\n",
    "            print(\"{} \\t {}\\n\".format(token, pos_info[\"pos\"]))\n",
    "        except Exception as e:\n",
    "            print(\"Expanded form for {} not available (error: {})\".format(pos_tag, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TreeTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TreeTagger` is available as a command line tool, but there are ways of calling it from within Python code.\n",
    "\n",
    "The code below uses a *python wrapper*, namely a couple of python classes/methods that exposes `TreeTagger`'s functionalities via Python objects/methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treetagger import TreeTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TREETAGGER_HOME\"] = os.path.expanduser('~/tree-tagger/cmd/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TreeTagger(language=\"latin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Cogito', 'V:IMP', 'cogo'],\n",
       " ['ergo', 'ADV', 'ergo'],\n",
       " ['sum', 'ESSE:IND', 'sum']]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tag(\"Cogito ergo sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['91', 'ADJ:NUM', '@card@'],\n",
       " ['92', 'ADJ:NUM', '@card@'],\n",
       " ['93', 'ADJ:NUM', '@card@'],\n",
       " ['94', 'ADJ:NUM', '@card@'],\n",
       " ['95', 'ADJ:NUM', '@card@'],\n",
       " ['96', 'ADJ:NUM', '@card@'],\n",
       " ['97', 'ADJ:NUM', '@card@'],\n",
       " ['98', 'ADJ:NUM', '@card@'],\n",
       " ['99', 'ADJ:NUM', '@card@'],\n",
       " ['100', 'ADJ:NUM', '@card@'],\n",
       " ['101', 'ADJ:NUM', '@card@'],\n",
       " ['102', 'ADJ:NUM', '@card@'],\n",
       " ['103', 'ADJ:NUM', '@card@'],\n",
       " ['104', 'ADJ:NUM', '@card@'],\n",
       " ['[', 'PUN', '['],\n",
       " ['1', 'ADJ:NUM', '@card@'],\n",
       " [']', 'PUN', ']'],\n",
       " ['Q.', 'ABBR', 'Q.'],\n",
       " ['Mucius', 'ADJ', '<unknown>'],\n",
       " ['augur', 'N:nom', 'augur'],\n",
       " ['multa', 'ADJ', 'multus'],\n",
       " ['narrare', 'V:INF', 'narro'],\n",
       " ['de', 'PREP', 'de'],\n",
       " ['C.', 'ABBR', 'C.'],\n",
       " ['Laelio', 'N:abl', '<unknown>'],\n",
       " ['socero', 'N:abl', 'socer'],\n",
       " ['suo', 'POSS', 'suus'],\n",
       " ['memoriter', 'ADV', 'memoriter'],\n",
       " ['et', 'CC', 'et'],\n",
       " ['iucunde', 'ADJ', '<unknown>'],\n",
       " ['solebat', 'V:IND', 'soleo'],\n",
       " ['nec', 'CC', 'nec'],\n",
       " ['dubitare', 'V:INF', 'dubito'],\n",
       " ['illum', 'DIMOS', 'ille'],\n",
       " ['in', 'PREP', 'in'],\n",
       " ['omni', 'PRON', 'omnis'],\n",
       " ['sermone', 'N:abl', 'sermo'],\n",
       " ['appellare', 'V:INF', 'appello'],\n",
       " ['sapientem', 'N:acc', 'sapiens'],\n",
       " [';', 'SENT', ';'],\n",
       " ['ego', 'PRON', 'ego'],\n",
       " ['autem', 'ADV', 'autem'],\n",
       " ['a', 'PREP', 'a'],\n",
       " ['patre', 'N:abl', 'pater'],\n",
       " ['ita', 'ADV', 'ita'],\n",
       " ['eram', 'ESSE:IND', 'sum'],\n",
       " ['deductus', 'V:PTC:nom', 'deduco'],\n",
       " ['ad', 'PREP', 'ad'],\n",
       " ['Scaevolam', 'NPR', '<unknown>'],\n",
       " ['sumpta', 'V:PTC:nom', 'sumo']]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tag(amicitia_words[100:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLTK taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grk_corpus_importer.import_corpus(\"greek_models_cltk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tag.pos import POSTag\n",
    "tagger = POSTag('greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('θεοὺς', 'N-P---MA-'),\n",
       " ('μὲν', 'G--------'),\n",
       " ('αἰτῶ', 'V1SPIA---'),\n",
       " ('τῶνδ', 'P-P---MG-'),\n",
       " ('᾽', None),\n",
       " ('ἀπαλλαγὴν', 'N-S---FA-'),\n",
       " ('πόνων', 'N-P---MG-'),\n",
       " ('φρουρᾶς', 'N-S---FG-'),\n",
       " ('ἐτείας', 'A-S---FG-'),\n",
       " ('μῆκος', 'N-S---NA-')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_ngram_123_backoff('θεοὺς μὲν αἰτῶ τῶνδ᾽ ἀπαλλαγὴν πόνων φρουρᾶς ἐτείας μῆκος')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('θεοὺς', 'N-P---MA-'),\n",
       " ('μὲν', 'G--------'),\n",
       " ('αἰτῶ', 'V1SPIA---'),\n",
       " ('τῶνδ', 'P-P---NG-'),\n",
       " ('᾽', 'Unk'),\n",
       " ('ἀπαλλαγὴν', 'N-S---FA-'),\n",
       " ('πόνων', 'N-P---MG-'),\n",
       " ('φρουρᾶς', 'N-S---FG-'),\n",
       " ('ἐτείας', 'A-S---FG-'),\n",
       " ('μῆκος', 'N-S---NA-')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_tnt('θεοὺς μὲν αἰτῶ τῶνδ᾽ ἀπαλλαγὴν πόνων φρουρᾶς ἐτείας μῆκος')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small detour: importing a Greek corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Is there a handy way to import an Ancient Greek corpus as we did for Latin?\n",
    "\n",
    "**A**: not quite (yet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step: convert the Perseus data (TEI/XML) into plain text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [03:44<00:00,  3.68s/it]\n"
     ]
    }
   ],
   "source": [
    "# my modified version of https://github.com/cltk/greek_text_perseus/blob/master/perseus_compiler.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import bleach\n",
    "#from cltk.corpus.classical_greek.replacer import Replacer\n",
    "from cltk.corpus.greek.beta_to_unicode import Replacer\n",
    "\n",
    "\n",
    "home = os.path.expanduser('~')\n",
    "cltk_path = os.path.join(home, 'cltk_data')\n",
    "#print(cltk_path)\n",
    "perseus_root = cltk_path + '/greek/text/greek_text_perseus/'\n",
    "#print(perseus_root)\n",
    "ignore = [\n",
    "    '.git',\n",
    "    'LICENSE.md',\n",
    "    'README.md',\n",
    "    'cltk_json',\n",
    "    'json',\n",
    "    'perseus_compiler.py'\n",
    "]\n",
    "authors = [d for d in os.listdir(perseus_root) if d not in ignore]\n",
    "\n",
    "for author in tqdm(authors):\n",
    "    texts = os.listdir(perseus_root + author + '/opensource')\n",
    "    for text in texts:\n",
    "        text_match = re.match(r'.*_gk.xml', text)\n",
    "        if text_match:\n",
    "            gk_file = text_match.group()\n",
    "            txt_file = perseus_root + author + '/opensource/' + gk_file\n",
    "            with open(txt_file) as gk:\n",
    "                html = gk.read()\n",
    "                beta_code = bleach.clean(html, strip=True).upper()\n",
    "                a_replacer = Replacer()\n",
    "                unicode_converted = a_replacer.beta_code(beta_code)\n",
    "                #print(unicode_converted)\n",
    "                unicode_root = cltk_path + '/greek/text/perseus_unicode/'\n",
    "                unic_pres = os.path.isdir(unicode_root)\n",
    "                if unic_pres is True:\n",
    "                    pass\n",
    "                else:\n",
    "                    os.mkdir(unicode_root)\n",
    "                author_path = unicode_root + author\n",
    "                author_path_pres = os.path.isdir(author_path)\n",
    "                if author_path_pres is True:\n",
    "                    pass\n",
    "                else:\n",
    "                    os.mkdir(author_path)\n",
    "                gk_file_txt = os.path.splitext(gk_file)[0] + '.txt'\n",
    "                uni_write = author_path + '/' + gk_file_txt\n",
    "                #print(uni_write)\n",
    "                with open(uni_write, 'w') as uni_write:\n",
    "                    uni_write.write(unicode_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "from cltk.tokenize.word import WordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "agrk_word_tokenizer = WordTokenizer('greek')\n",
    "agrk_sentence_tokenizer = TokenizeSentence(\"greek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "cltk_path = os.path.expanduser('~/cltk_data')\n",
    "try:\n",
    "    perseusgreek = PlaintextCorpusReader(\n",
    "        cltk_path + '/greek/text/perseus_unicode/', \n",
    "        '.*\\.txt',\n",
    "        word_tokenizer=agrk_word_tokenizer, \n",
    "        sent_tokenizer=agrk_sentence_tokenizer, \n",
    "        encoding='utf-8'\n",
    "    )    \n",
    "    pass\n",
    "except IOError as e:\n",
    "    pass\n",
    "    # print(\"Corpus not found. Please check that the Latin Library is installed in CLTK_DATA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PlaintextCorpusReader in '/Users/rromanello/cltk_data/greek/text/perseus_unicode'>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perseusgreek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds = perseusgreek.words('Aristophanes/aristoph.birds_gk.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ἐπέγειρον', 'αὐτόν', '.', 'Θεράπων', 'Ἔποποσ', 'οἶδα', 'μὲν', 'σαφῶσ', 'ὅτι', 'ἀχθέσεται', ',', 'σφῷν', 'δ’', 'αὐτὸν', 'οὕνεκ’', 'ἐπεγερῶ', '.', 'Πισθέταιροσ', 'κακῶς', 'σύ', 'γ’', 'ἀπόλοῐ', ',', 'ὥς', 'μ’', 'ἀπέκτεινας', 'δέει', '.', 'Ἐυελπίδησ', 'οἴμοι', 'κακοδαίμων', 'χὠ', 'κολοιός', 'μοἴχεται', 'ὑπὸ', 'τοῦ', 'δέους\\\\', '.', 'Πισθέταιροσ', 'ὦ', 'δειλότατον', 'σὺ', 'θηρίον', ',', 'δείσας', 'ἀφῆκας', 'τὸν', 'κολοιόν', ';', 'Ἐυελπίδησ', 'εἰπέ', 'μοι', ',', 'σὺ', 'δὲ', 'τὴν', 'κορώνην', 'οὐκ', 'ἀφῆκας', 'καταπεσών', ';', 'Πισθέταιροσ', 'μὰ', 'Δί’', 'οὐκ', 'ἔγωγε', '.', 'Ἐυελπίδησ', 'ποῦ', 'γάρ', 'ἐστ’', ';', 'Πισθέταιροσ', 'ἀπέπτετο', '.', 'Ἐυελπίδησ', 'οὐκ', 'ἆρ’', 'ἀφῆκας', ';', 'ὦγάθ’', 'ὡς', 'ἀνδρεῖος', 'εἶ', '.', 'Ἔποψ', 'ἄνοιγε', 'τὴν', 'ὕλην', ',', 'ἵν’', 'ἐξέλθω', 'ποτέ', '.', 'Ἐυελπίδησ', 'ὦ', 'Ἡράκλεις', 'τουτὶ', 'τί', 'ποτ’']\n"
     ]
    }
   ],
   "source": [
    "print(list(birds[1000:1100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ἐπέγειρον', 'Unk'),\n",
       " ('αὐτόν', 'A-S---MA-'),\n",
       " ('.', 'U--------'),\n",
       " ('Θεράπων', 'Unk'),\n",
       " ('Ἔποποσ', 'Unk'),\n",
       " ('οἶδα', 'V1SRIA---'),\n",
       " ('μὲν', 'G--------'),\n",
       " ('σαφῶσ', 'Unk'),\n",
       " ('ὅτι', 'C--------'),\n",
       " ('ἀχθέσεται', 'Unk'),\n",
       " (',', 'U--------'),\n",
       " ('σφῷν', 'P-D---MG-'),\n",
       " ('δ', 'G--------'),\n",
       " ('’', 'Unk'),\n",
       " ('αὐτὸν', 'A-S---MA-'),\n",
       " ('οὕνεκ', 'C--------'),\n",
       " ('’', 'Unk'),\n",
       " ('ἐπεγερῶ', 'Unk'),\n",
       " ('.', 'U--------'),\n",
       " ('Πισθέταιροσ', 'Unk'),\n",
       " ('κακῶς', 'D--------'),\n",
       " ('σύ', 'P-S----N-'),\n",
       " ('γ', 'G--------'),\n",
       " ('’', 'Unk'),\n",
       " ('ἀπόλοῐ', 'Unk'),\n",
       " (',', 'U--------'),\n",
       " ('ὥς', 'C--------'),\n",
       " ('μ', 'P-S---MA-'),\n",
       " ('’', 'Unk'),\n",
       " ('ἀπέκτεινας', 'Unk'),\n",
       " ('δέει', 'N-S---ND-'),\n",
       " ('.', 'U--------'),\n",
       " ('Ἐυελπίδησ', 'Unk'),\n",
       " ('οἴμοι', 'E--------'),\n",
       " ('κακοδαίμων', 'Unk'),\n",
       " ('χὠ', 'L-S---MN-'),\n",
       " ('κολοιός', 'Unk'),\n",
       " ('μοἴχεται', 'Unk'),\n",
       " ('ὑπὸ', 'R--------'),\n",
       " ('τοῦ', 'L-S---NG-'),\n",
       " ('δέους', 'N-S---NG-'),\n",
       " ('\\\\', 'Unk'),\n",
       " ('.', 'U--------'),\n",
       " ('Πισθέταιροσ', 'Unk'),\n",
       " ('ὦ', 'E--------'),\n",
       " ('δειλότατον', 'Unk'),\n",
       " ('σὺ', 'P-S----N-'),\n",
       " ('θηρίον', 'N-S---NN-'),\n",
       " (',', 'U--------'),\n",
       " ('δείσας', 'T-SAPAMN-'),\n",
       " ('ἀφῆκας', 'V2SAIA---'),\n",
       " ('τὸν', 'L-S---MA-'),\n",
       " ('κολοιόν', 'Unk'),\n",
       " (';', 'U--------'),\n",
       " ('Ἐυελπίδησ', 'Unk'),\n",
       " ('εἰπέ', 'V2SAMA---'),\n",
       " ('μοι', 'P-S---MD-'),\n",
       " (',', 'U--------'),\n",
       " ('σὺ', 'P-S---MN-'),\n",
       " ('δὲ', 'G--------'),\n",
       " ('τὴν', 'L-S---FA-'),\n",
       " ('κορώνην', 'N-S---FA-'),\n",
       " ('οὐκ', 'D--------'),\n",
       " ('ἀφῆκας', 'V2SAIA---'),\n",
       " ('καταπεσών', 'Unk'),\n",
       " (';', 'U--------'),\n",
       " ('Πισθέταιροσ', 'Unk'),\n",
       " ('μὰ', 'G--------'),\n",
       " ('Δί', 'Unk'),\n",
       " ('’', 'Unk'),\n",
       " ('οὐκ', 'D--------'),\n",
       " ('ἔγωγε', 'P-S---MN-'),\n",
       " ('.', 'U--------'),\n",
       " ('Ἐυελπίδησ', 'Unk'),\n",
       " ('ποῦ', 'D--------'),\n",
       " ('γάρ', 'G--------'),\n",
       " ('ἐστ', 'V3SPIA---'),\n",
       " ('’', 'Unk'),\n",
       " (';', 'U--------'),\n",
       " ('Πισθέταιροσ', 'Unk'),\n",
       " ('ἀπέπτετο', 'Unk'),\n",
       " ('.', 'U--------'),\n",
       " ('Ἐυελπίδησ', 'Unk'),\n",
       " ('οὐκ', 'D--------'),\n",
       " ('ἆρ', 'D--------'),\n",
       " ('’', 'Unk'),\n",
       " ('ἀφῆκας', 'V2SAIA---'),\n",
       " (';', 'U--------'),\n",
       " ('ὦγάθ', 'Unk'),\n",
       " ('’', 'Unk'),\n",
       " ('ὡς', 'D--------'),\n",
       " ('ἀνδρεῖος', 'Unk'),\n",
       " ('εἶ', 'V2SPIA---'),\n",
       " ('.', 'U--------'),\n",
       " ('Ἔποψ', 'Unk'),\n",
       " ('ἄνοιγε', 'V2SPMA---'),\n",
       " ('τὴν', 'L-S---FA-'),\n",
       " ('ὕλην', 'N-S---FA-'),\n",
       " (',', 'U--------'),\n",
       " ('ἵν', 'C--------'),\n",
       " ('’', 'Unk'),\n",
       " ('ἐξέλθω', 'Unk'),\n",
       " ('ποτέ', 'G--------'),\n",
       " ('.', 'U--------'),\n",
       " ('Ἐυελπίδησ', 'Unk'),\n",
       " ('ὦ', 'E--------'),\n",
       " ('Ἡράκλεις', 'Unk'),\n",
       " ('τουτὶ', 'Unk'),\n",
       " ('τί', 'P-S---NA-'),\n",
       " ('ποτ', 'G--------'),\n",
       " ('’', 'Unk')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_tnt(\" \".join(birds[1000:1100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TreeTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Present two examples: one suitable for automatic lemmatization and the other more as a support for the reader/annotator.\n",
    "\n",
    "Main difference: how words that may correspond to multiple lemmata are handled.\n",
    "\n",
    "### Latin\n",
    "\n",
    "#### CLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.utils.file_operations import open_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training sentences\n",
    "rel_path = os.path.join('~/cltk_data/latin/model/latin_models_cltk/lemmata/backoff')\n",
    "path = os.path.expanduser(rel_path)\n",
    "\n",
    "# Check for presence of latin_pos_lemmatized_sents\n",
    "file = 'latin_pos_lemmatized_sents.pickle' \n",
    "latin_pos_lemmatized_sents_path = os.path.join(path, file)\n",
    "if os.path.isfile(latin_pos_lemmatized_sents_path):\n",
    "    latin_pos_lemmatized_sents = open_pickle(latin_pos_lemmatized_sents_path)\n",
    "else:\n",
    "    latin_pos_lemmatized_sents = []\n",
    "    print('The file %s is not available in cltk_data' % file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "amicitia_sents = latinlibrary.sents('cicero/amic.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " '4',\n",
       " ']',\n",
       " 'Cum',\n",
       " 'enim',\n",
       " 'saepe',\n",
       " 'cum',\n",
       " 'me',\n",
       " 'ageres',\n",
       " 'ut',\n",
       " 'de',\n",
       " 'amicitia',\n",
       " 'scriberem',\n",
       " 'aliquid',\n",
       " ',',\n",
       " 'digna',\n",
       " 'mihi',\n",
       " 'res',\n",
       " 'cum',\n",
       " 'omnium',\n",
       " 'cognitione',\n",
       " 'tum',\n",
       " 'nostra',\n",
       " 'familiaritate',\n",
       " 'visa',\n",
       " 'est',\n",
       " '.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amicitia_sents[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "backoff_lemmatizer = BackoffLatinLemmatizer(train=latin_pos_lemmatized_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 'punc'),\n",
       " ('4', '4'),\n",
       " (']', 'punc'),\n",
       " ('Cum', 'Cos2'),\n",
       " ('enim', 'enim'),\n",
       " ('saepe', 'saepe'),\n",
       " ('cum', 'cum2'),\n",
       " ('me', 'ego'),\n",
       " ('ageres', 'ago'),\n",
       " ('ut', 'ut'),\n",
       " ('de', 'de'),\n",
       " ('amicitia', 'amicitia'),\n",
       " ('scriberem', 'scribo'),\n",
       " ('aliquid', 'aliquis'),\n",
       " (',', 'punc'),\n",
       " ('digna', 'dignus'),\n",
       " ('mihi', 'ego'),\n",
       " ('res', 'res'),\n",
       " ('cum', 'cum2'),\n",
       " ('omnium', 'omnis'),\n",
       " ('cognitione', 'cognitio'),\n",
       " ('tum', 'tum'),\n",
       " ('nostra', 'noster'),\n",
       " ('familiaritate', 'familiaritas'),\n",
       " ('visa', 'video'),\n",
       " ('est', 'sum'),\n",
       " ('.', 'punc')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backoff_lemmatizer.lemmatize(amicitia_sents[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But behind the scenes, this is what is going on:\n",
    "\n",
    "```python\n",
    "\n",
    "    def _define_lemmatizer(self):\n",
    "        # Suggested backoff chain--should be tested for optimal order\n",
    "        backoff0 = None\n",
    "        backoff1 = IdentityLemmatizer()\n",
    "        backoff2 = TrainLemmatizer(model=self.LATIN_OLD_MODEL, backoff=backoff1)\n",
    "        backoff3 = PPLemmatizer(regexps=self.latin_verb_patterns, pps=self.latin_pps, backoff=backoff2)                 \n",
    "        backoff4 = RegexpLemmatizer(self.latin_sub_patterns, backoff=backoff3)\n",
    "        backoff5 = UnigramLemmatizer(self.train_sents, backoff=backoff4)\n",
    "        backoff6 = TrainLemmatizer(model=self.LATIN_MODEL, backoff=backoff5)      \n",
    "        #backoff7 = BigramPOSLemmatizer(self.pos_train_sents, include=['cum'], backoff=backoff6)\n",
    "        #lemmatizer = backoff7\n",
    "        lemmatizer = backoff6\n",
    "        return lemmatizer\n",
    "\n",
    "```\n",
    "\n",
    "further readings: \n",
    "* https://github.com/cltk/cltk/blob/master/cltk/lemmatize/latin/backoff.py\n",
    "* https://disiectamembra.wordpress.com/2016/08/23/wrapping-up-google-summer-of-code/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyCollatinus\n",
    "\n",
    "* Python port of the [Collatinus lemmatizer](https://github.com/biblissima/collatinus)\n",
    "* good if you can read some French (or at least practice it) 😉\n",
    "* the PoS tags used by Collatinus are explained [here](https://github.com/biblissima/collatinus/blob/master/NOTES_Tagger.md)\n",
    "* morphological analysis not readily machine readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import and instantiate the PyCollatinus lemmatizer (`Lemmatiseur`) – and ignore the long list of warnings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: honor has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: aer has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: tethys has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: opes has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: dos has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: corpus has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: ciuis has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: thales has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: poesis has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: manes has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: turris has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: uis has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: nauis has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: apis has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: mare has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: moenia has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: animal has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: mille has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n",
      "/Users/mat/.local/share/virtualenvs/sunoikisis_dc-I3iKJ3Z3/lib/python3.6/site-packages/pycollatinus/parser.py:335: MissingRadical: ille has no radical 1\n",
      "  warnings.warn(gr + \" has no radical {}\".format(numRad), MissingRadical)\n"
     ]
    }
   ],
   "source": [
    "from pycollatinus import Lemmatiseur\n",
    "analyzer = Lemmatiseur()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmatiser can take as input a **single word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'desinence': 'ito',\n",
       "  'form': 'cogito',\n",
       "  'lemma': 'cogo',\n",
       "  'morph': '2ème singulier impératif futur actif',\n",
       "  'radical': 'cog'},\n",
       " {'desinence': 'ito',\n",
       "  'form': 'cogito',\n",
       "  'lemma': 'cogo',\n",
       "  'morph': '3ème singulier impératif futur actif',\n",
       "  'radical': 'cog'},\n",
       " {'desinence': 'o',\n",
       "  'form': 'cogito',\n",
       "  'lemma': 'cogito',\n",
       "  'morph': '1ère singulier indicatif présent actif',\n",
       "  'radical': 'cogit'},\n",
       " {'desinence': 'o',\n",
       "  'form': 'cogito',\n",
       "  'lemma': 'cogito',\n",
       "  'morph': '1ère singulier indicatif présent actif',\n",
       "  'radical': 'cogit'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(analyzer.lemmatise(\"Cogito\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or an **entire sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'desinence': 'ito',\n",
       "   'form': 'cogito',\n",
       "   'lemma': 'cogo',\n",
       "   'morph': '2ème singulier impératif futur actif',\n",
       "   'radical': 'cog'},\n",
       "  {'desinence': 'ito',\n",
       "   'form': 'cogito',\n",
       "   'lemma': 'cogo',\n",
       "   'morph': '3ème singulier impératif futur actif',\n",
       "   'radical': 'cog'},\n",
       "  {'desinence': 'o',\n",
       "   'form': 'cogito',\n",
       "   'lemma': 'cogito',\n",
       "   'morph': '1ère singulier indicatif présent actif',\n",
       "   'radical': 'cogit'},\n",
       "  {'desinence': 'o',\n",
       "   'form': 'cogito',\n",
       "   'lemma': 'cogito',\n",
       "   'morph': '1ère singulier indicatif présent actif',\n",
       "   'radical': 'cogit'}],\n",
       " [{'desinence': 'o',\n",
       "   'form': 'ergo',\n",
       "   'lemma': 'ergo',\n",
       "   'morph': '1ère singulier indicatif présent actif',\n",
       "   'radical': 'erg'},\n",
       "  {'desinence': '',\n",
       "   'form': 'ergo',\n",
       "   'lemma': 'ergo',\n",
       "   'morph': '-',\n",
       "   'radical': 'ergo'},\n",
       "  {'desinence': '',\n",
       "   'form': 'ergo',\n",
       "   'lemma': 'ergo',\n",
       "   'morph': 'positif',\n",
       "   'radical': 'ergo'}],\n",
       " [{'desinence': 'um',\n",
       "   'form': 'sum',\n",
       "   'lemma': 'sum',\n",
       "   'morph': '1ère singulier indicatif présent actif',\n",
       "   'radical': 's'}]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(analyzer.lemmatise_multiple(\"Cogito ergo sum\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to output the lemmatisation in a more intellegible way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\tcogito\t\tcogo 2ème singulier impératif futur actif\n",
      "1.2\tcogito\t\tcogo 3ème singulier impératif futur actif\n",
      "1.3\tcogito\t\tcogito 1ère singulier indicatif présent actif\n",
      "1.4\tcogito\t\tcogito 1ère singulier indicatif présent actif\n",
      "2.1\tergo\t\tergo 1ère singulier indicatif présent actif\n",
      "2.2\tergo\t\tergo -\n",
      "2.3\tergo\t\tergo positif\n",
      "3.1\tsum\t\tsum 1ère singulier indicatif présent actif\n"
     ]
    }
   ],
   "source": [
    "# the analyzer output is essentially a list of lists\n",
    "# for each analyzed token it returns a list of possible lemmata\n",
    "# here we iterate through both lists and display the analysis as we go along\n",
    "\n",
    "for n, result in enumerate(analyzer.lemmatise_multiple(\"Cogito ergo sum\")):\n",
    "    for i, lemma in enumerate(result):\n",
    "        print(\n",
    "            \"{}.{}\\t{}\\t\\t{} {}\".format(\n",
    "                n + 1,\n",
    "                i + 1,\n",
    "                lemma[\"form\"],\n",
    "                lemma[\"lemma\"],\n",
    "                lemma[\"morph\"]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same but also with PoS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\tcogito\tv\tcogo 2ème singulier impératif futur actif\n",
      "1.2\tcogito\tv\tcogo 3ème singulier impératif futur actif\n",
      "1.3\tcogito\tv\tcogito 1ère singulier indicatif présent actif\n",
      "1.4\tcogito\tv\tcogito 1ère singulier indicatif présent actif\n",
      "2.1\tergo\tv\tergo 1ère singulier indicatif présent actif\n",
      "2.2\tergo\tc\tergo -\n",
      "2.3\tergo\td\tergo positif\n",
      "3.1\tsum\tv\tsum 1ère singulier indicatif présent actif\n"
     ]
    }
   ],
   "source": [
    "# the analyzer output is essentially a list of lists\n",
    "# for each analyzed token it returns a list of possible lemmata\n",
    "# here we iterate through both lists and display the analysis as we go along\n",
    "\n",
    "for n, result in enumerate(analyzer.lemmatise_multiple(\"Cogito ergo sum\", pos=True),):\n",
    "    for i, lemma in enumerate(result):\n",
    "        print(\n",
    "            \"{}.{}\\t{}\\t{}\\t{} {}\".format(\n",
    "                n + 1,\n",
    "                i + 1,\n",
    "                lemma[\"form\"],\n",
    "                lemma[\"pos\"],\n",
    "                lemma[\"lemma\"],\n",
    "                lemma[\"morph\"]\n",
    "            )\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "621px",
    "left": "0px",
    "right": "1169px",
    "top": "111px",
    "width": "234px"
   },
   "toc_section_display": true,
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
